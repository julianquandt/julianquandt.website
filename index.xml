<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Julian Quandt</title>
    <link>https://julianquandt.com/</link>
      <atom:link href="https://julianquandt.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Julian Quandt</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 16 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Julian Quandt</title>
      <link>https://julianquandt.com/</link>
    </image>
    
    <item>
      <title>Power Analysis by Data Simulation in R - Part III</title>
      <link>https://julianquandt.com/post/power-analysis-by-data-simulation-in-r-part-iii/</link>
      <pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://julianquandt.com/post/power-analysis-by-data-simulation-in-r-part-iii/</guid>
      <description>
&lt;script src=&#34;https://julianquandt.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-power-analysis-by-simulation-in-r-for-really-any-design---part-iii&#34;&gt;The Power Analysis by simulation in &lt;code&gt;R&lt;/code&gt; for really any design - Part III&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#footnotes&#34;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;html&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;https://julianquandt.com/css/style.css&#34; /&gt;
&lt;/html&gt;
&lt;div class=&#34;notebox&#34;&gt;
&lt;p&gt;
&lt;p&gt;&lt;strong&gt;Click &lt;a href=&#34;https://julianquandt.com/rmd_files/2020-06-16-power-analysis-by-data-simulation-in-r-part-iii.Rmd%22&#34;&gt;HERE&lt;/a&gt; to download the .Rmd file&lt;/strong&gt;&lt;/p&gt;
&lt;em&gt;This blog is also available on &lt;a href=&#34;https://www.r-bloggers.com/&#34;&gt;R-Bloggers&lt;/a&gt;&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;style type=&#34;text/css&#34;&gt;
button.btn.collapsed:before
{
    content:&#39;+&#39; ;
    display:block;
    width:15px;
}
button.btn:before
{
    content:&#39;-&#39; ;
    display:block;
    width:15px;
}
&lt;/style&gt;
&lt;div id=&#34;the-power-analysis-by-simulation-in-r-for-really-any-design---part-iii&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Power Analysis by simulation in &lt;code&gt;R&lt;/code&gt; for really any design - Part III&lt;/h1&gt;
&lt;p&gt;This is Part III of my tutorial on how to do power-analysis by simulation.
In Part I, we saw how to do a simulation for a simple toy-example with a coin-toss.
In part II we learned how to simulate univariate and multivariate normal-distributions, specify our assumptions about the expected effect and test our simulated data with different t-tests.
In this part, we will focus on how we can write a flexible test for any model by making use of the fact that many statistical standard methods (e.g. t-test, ANOVA) can be rewritten as a linear model.
This way, no matter what situation we find ourselves in, we will always be able to use the same power-simulation with only slight changes.&lt;/p&gt;
&lt;p&gt;In part IV we will learn how to apply this technique to complicated designs such as linear mixed-effects models and generalized mixed-effects models.&lt;/p&gt;
&lt;div id=&#34;revisiting-the-t-test-just-another-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Revisiting the t-test: just another linear model&lt;/h2&gt;
&lt;p&gt;Before I even start with this topic, I want to point out that I took much of the information I present here from Jonas Kristoffer Lindelov’s &lt;strong&gt;fantastic&lt;/strong&gt; blog post &lt;a href=&#34;https://lindeloev.github.io/tests-as-linear/&#34;&gt;“Common statistical tests are linear models (or: how to teach stats)”&lt;/a&gt;.
If you have not read it yet, you should definitely do so, either now or after you finished this part of the tutorial.
Seriously, it is a great blog-post that inspired me to write up this part of the tutorial in a similar way in which we want to learn one flexible method of data-simulation rather than changing what we are doing for each different situation.&lt;/p&gt;
&lt;p&gt;Ok so what does it mean to say that, for instance, a t-test is just a linear model?
First of all, here is a quick reminder about the linear model with a quick example:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y_i = \beta_0 + \beta_1x_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above formula states that each value of y (the little i can be read as “each”) can be described by adding a value &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and the product of &lt;span class=&#34;math inline&#34;&gt;\(beta_0 \times x\)&lt;/span&gt;.
Now, how does this relate to the t-test? Again, if you want a more in-depth discussion read the blog-post above.
To demonstrate the point, lets revisit the example from part II of this tutorial.
We had 2 groups and we assumed that one group can be described as &lt;code&gt;group1 = normal(0,2)&lt;/code&gt; and the other group can be described as &lt;code&gt;group2 = normal(1,2)&lt;/code&gt;.
Now if we want to test whether the difference in means is significant, we can run a independent-samples t-test.
However, if we rephrase the problem what we want to know whether group membership (whether you belong to group1 or group2; the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; variable in this case) significantly &lt;em&gt;predicts&lt;/em&gt; the person’s score (y in this case).
To do so, we can just assume let &lt;span class=&#34;math inline&#34;&gt;\(beta_0\)&lt;/span&gt; describe the mean of, say, group1 in this case.
If we do this and if we re-write model above to predict the mean of group1, it looks the following way:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
M_{group1} = M_{group1}+\beta_1\times x
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What should &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; be in this case?
Well, as we already have the mean of group1 on the right side in the term that was previously &lt;span class=&#34;math inline&#34;&gt;\(beta_0\)&lt;/span&gt;, we do not want to add &lt;em&gt;anything&lt;/em&gt; to it.
Thus, if someone member from group1, to predict the mean-score we should have an observed value for group-membership of 0.
This way, the formula becomes&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
M_{group1} = M_{group1}+\beta_1\times 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; term becomes 0 entirely. Note that we of course do not know the mean of group1 as we will &lt;em&gt;simulate&lt;/em&gt; it.
&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(M_{group1}\)&lt;/span&gt; here) is a &lt;em&gt;parameter&lt;/em&gt;, i.e. something that we want the model to estimate because we do not know it.&lt;/p&gt;
&lt;p&gt;What if we want to describe the mean of a person in group2?
We can do this by just adding something to the mean of group1, namely the &lt;em&gt;difference&lt;/em&gt; between the two groups.
In other words, we can describe the mean of group2 by saying its the mean of group1 + the difference of their means times x:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
M_{group2} = M_{group1}+(M_{group2}-M_{group1})\times x
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What should &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; be now?
Just like &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, our new mean-difference parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is an unknown that depends on our simulated means and will be estimated by the model.
&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on the other hand should be 1, because if someone is from group2 we want the predicted mean of that person to be the mean of group1 + &lt;strong&gt;1 times&lt;/strong&gt; the difference between group1 and group2.&lt;/p&gt;
&lt;p&gt;Therefore if someone is from group2 our model becomes:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
M_{group2} = M_{group1}+(M_{group2}-M_{group1})\times 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What we just did is so-called &lt;em&gt;dummy coding&lt;/em&gt;, i.e. we made a new dummy variable that tells us someone’s group membership:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0 if the person is in group1&lt;/li&gt;
&lt;li&gt;1 if the person is in group2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we use this dummy variable in the linear model we can do exactly the same test that we did with the t-test earlier.
Let me demonstrate with the exact same groups that we simulated in the beginning of part II of this tutorial.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
group1 &amp;lt;- rnorm(30, 1, 2)
group2 &amp;lt;- rnorm(30, 0, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can run the independent-samples t-test again, yielding the same results as in part II.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(group1, group2, paired = FALSE, var.equal = TRUE, conf.level = 0.9)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Two Sample t-test
## 
## data:  group1 and group2
## t = 3.1402, df = 58, p-value = 0.002656
## alternative hypothesis: true difference in means is not equal to 0
## 90 percent confidence interval:
##  0.7064042 2.3143690
## sample estimates:
## mean of x mean of y 
##  0.407150 -1.103237&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To do our linear model analysis instead, we first have to create a data-frame that contains the 2 groups and tells us about their group membership.
In other words we will make a data-set that has the variables &lt;code&gt;score&lt;/code&gt; which are the values in the vectors of group1 and group2 and a variable &lt;code&gt;group&lt;/code&gt; that will be 0 if a person is from group1 and 1 if the person is from group2.
In this case, after putting the data together, we create a dummy variable with &lt;code&gt;ifelse&lt;/code&gt; a function that you can read like: if the group variable has value “group1” in a certain row, then make dummy_group in that row = 0, else make it 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(group1) &amp;lt;- rep(&amp;quot;group1&amp;quot;, length(group1)) # name the vectors to use the names as the group variable
names(group2) &amp;lt;- rep(&amp;quot;group2&amp;quot;, length(group2))

lm_groupdat &amp;lt;- data.frame(score = c(group1,group2), group = c(names(group1), names(group2))) # make data-set from scores and names

lm_groupdat$dummy_group &amp;lt;- ifelse(lm_groupdat$group == &amp;quot;group1&amp;quot;, 0, 1) # create dummy variable &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can use this to run our linear model with the &lt;code&gt;lm&lt;/code&gt; function.
In case you have not used it before, we indicate the &lt;em&gt;intercept&lt;/em&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(beta_0\)&lt;/span&gt; with a 1 in this case, to tell the model that we want 1 times &lt;span class=&#34;math inline&#34;&gt;\(beta_0\)&lt;/span&gt; in there and tell it that we want dummy_group times the difference between the groups, &lt;span class=&#34;math inline&#34;&gt;\(beta_1\)&lt;/span&gt; in there.
The code looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(score ~ 1+ dummy_group, dat = lm_groupdat)) # use summary function to get p-values&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = score ~ 1 + dummy_group, data = lm_groupdat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.0985 -1.1327 -0.4088  0.7360  5.4245 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept)   0.4072     0.3401   1.197  0.23612   
## dummy_group  -1.5104     0.4810  -3.140  0.00266 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.863 on 58 degrees of freedom
## Multiple R-squared:  0.1453, Adjusted R-squared:  0.1306 
## F-statistic: 9.861 on 1 and 58 DF,  p-value: 0.002656&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First let us see what the estimates mean in this case.
The intercept has an estimate of .4072.
As we said earlier, this, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, should be the group-mean of group1 and indeed wee see that &lt;code&gt;mean(group1)&lt;/code&gt; = 0.40715 gives us the same result.
What is the second estimate, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; for dummy_group should yield the difference between the means.
We can confirm that &lt;code&gt;mean(group2)-mean(group1)&lt;/code&gt; = -1.5103866 gives us the same result in this case.
Further we see that the DFs, t-value and p-value are identical to the earlier t-test.&lt;/p&gt;
&lt;p&gt;This method would also work with the one-sample t-test approach and the paired-samples t-test approach from part II of the tutorial, but I will leave it as an exercise to the reader to try it out, as I do want to move on to other situations.
Again (cannot mention this enough) make sure you head to Jonas Kristoffer Lindelov’s &lt;a href=&#34;https://lindeloev.github.io/tests-as-linear/&#34;&gt;blog post&lt;/a&gt; if you want to see how to do this or if you want to read more on this topic.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-a-2x2-between-subject-anova-as-a-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating a 2x2 between-subject ANOVA (as a linear model)&lt;/h2&gt;
&lt;div id=&#34;how-to-specify-raw-effect-sizes-with-no-prior-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to specify raw effect-sizes with no prior data&lt;/h3&gt;
&lt;p&gt;In this part, I will give an example of how we could specify prior effect-sizes for our research design given we have not collected any prior data and are only working with assumptions.&lt;/p&gt;
&lt;p&gt;Ok time to move on to more sophisticated designs.
Imagine you are interested in students attitude towards smoking and how it depends on the medium of the message and the focus of the message.
More precisely we want to know whether people’s attitude is different after seeing a visual anti-smoking message (these pictures on the package) vs a text-message (the text belonging to that picture).
Moreover, we are interested in whether the attitude that people report is different after seeing a message that regards the consequences on other people (e.g. smoking can harm your loved ones) as compared to yourself (smoking can cause cancer).
Thus we have the following design&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: This is just an example that i made up so it does not reflect any real-life effect and one might easily argue a different way as this RQ is not based on any underlying theory but it allows me to demonstrate the point without further elaborating on any psychological theory.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;DV: attitude towards smoking (0-100)
IV1: medium (text vs. visual)
IV2: focus (internal vs. external)&lt;/p&gt;
&lt;p&gt;This is, there are 4 groups:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;group_TI will receive text-messages that are internal&lt;/li&gt;
&lt;li&gt;group_TE will receive text-messages that are external&lt;/li&gt;
&lt;li&gt;group_VI will receive visual messages that are internal&lt;/li&gt;
&lt;li&gt;group_VE will receive visual messages that are external&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lets assume that we expect that people’s attitude will be more negative after seeing a visual rather than text message if the focus is internal (i.e. the message is about yourself) because it might be difficult to imagine that oneself would get cancer after reading a text but seeing a picture might cause fear regardless.
For the external focus on the other hand, we expect a more negative attitude after reading a text as compared to seeing a picture, as it might have more impact on attitude to imagine a loved one get hurt than seeing a stranger in a picture suffering from the consequences of second-hand smoking.
Furthermore, we expect that the internal focus messages will be related to lower attitudes compared to the external focus messages on average but we expect no main-effect of picture vs. text-messages.&lt;/p&gt;
&lt;p&gt;Thus, we expect:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;no main-effect of medium&lt;/li&gt;
&lt;li&gt;main-effect of focus&lt;/li&gt;
&lt;li&gt;crossover-interaction of medium x focus&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this point, in my opinion it is more difficult to work with standardized effect-sizes like partial correlations e.g. in this case then just to specify everything on the response scale directly.
A first good step is to visualize some rough means that show the desired behavior that we described in words earlier and see where we are going, starting with the main-effects.&lt;/p&gt;
&lt;p&gt;We expect a main effect of focus so the means of the 2 internal groups should be lower (more negative attitude) than the means of the external groups.
For example, we could make the overall mean of the internal focus groups (group_TI and group_VI) 20 and the mean of the external groups (group_TE and group_VE) 50.
Note how this would already reflect the main-effect but also a belief that the smoking-attitudes are on average quite negative as we assume both means to be on the low end of the scale.
However, for now we just want to make a rough guess and we might adjust our expectations in a second.&lt;/p&gt;
&lt;p&gt;Ok so now what we have to do is specify the means for the media groups in a way that reflects our interaction hypothesis.
We could for example assume that the mean of group_TI is 30 while the mean of group_VI is 10 and we could assume that the mean of group_TE is 40 and the mean of group_VE is 60.&lt;/p&gt;
&lt;p&gt;This way, for the internal groups, visual messages are more negative than texts, while for the external groups texts are more negative than visual messages.&lt;/p&gt;
&lt;p&gt;We could plot this to see how it looks like&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;focus &amp;lt;- rep(c(&amp;quot;internal&amp;quot;, &amp;quot;external&amp;quot;), each = 2)
media &amp;lt;- rep(c(&amp;quot;text&amp;quot;, &amp;quot;visual&amp;quot;), times = 2)
mean_TI &amp;lt;- 50
mean_VI &amp;lt;- 20
mean_TE &amp;lt;- 30
mean_VE &amp;lt;- 60

pd &amp;lt;- data.frame(score = c(mean_TI, mean_VI, mean_TE, mean_VE), focus = focus, media = media)

interaction.plot(pd$focus, pd$media, pd$score, ylim = c(0,100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-06-16-power-analysis-by-data-simulation-in-r-part-iii_files/figure-html/plot_exp1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(pd$score[pd$focus == &amp;quot;internal&amp;quot;]) != mean(pd$score[pd$focus == &amp;quot;external&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(pd$score[pd$media == &amp;quot;text&amp;quot;]) == mean(pd$score[pd$media == &amp;quot;visual&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the above situation satisfies all things that we wanted to specify:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The means of the two focus groups are not the same so that internal are lower than external values if we pool over the media conditions.&lt;/li&gt;
&lt;li&gt;The mean for both media conditions are the same so there is no main-effect.&lt;/li&gt;
&lt;li&gt;The lines cross over showing the interaction that we wanted.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, three problems remain here.
First of all, the simple slope for the visual messages is steeper than the simple slope for the text messages, which is not something we specified in our hypotheses beforehand.
It rather comes as a side-effect of that we want our internal means to be lower than the external means but we also want the visual messages to be more impactful in the internal condition while our text-messages should be more impactful on the external conditions.
Do we find this plausible or do we think this would rather be the other way around?&lt;/p&gt;
&lt;div class=&#34;notebox&#34;&gt;
&lt;p&gt;
This a good point for a short reflection: You might already be overwhelmed how complicated power-analysis by simulation already is with these still relatively simple designs.
However, notice that e.g. the follow-up tests for the slopes are things that we &lt;em&gt;can&lt;/em&gt; care about but we do not &lt;em&gt;need&lt;/em&gt; to care about them.
Our interaction-hypothesis is mainly interested in whether the difference in scores between the visual and the text messages when comparing the internal focus group is the same as the difference for the external groups.
As this is clearly not the case in this example, we are good to go and test the interaction hypothesis.
However, if we clearly are interested in the simple slopes and have hypotheses about them, we &lt;em&gt;can&lt;/em&gt; put it in our simulation, and that of course urges us to think some more but it is a &lt;em&gt;great&lt;/em&gt; tool to see whether we actually thought about everything, as this might be a point that would have slipped our attention entirely when running a classical power-analysis and pre-registering the overall interaction only.
Now, we can think about it if we &lt;em&gt;want&lt;/em&gt; to or continue by saying that for now we only want to specify the interaction.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Let us assume that we are fine with the fact that visual messages differ somewhat more than text messages.
However, the numbers we have chosen so far are all but plausible.
In this case, if we want to come up with more plausible numbers it is good if, instead of thinking about means directly, we make assumptions about how each of the group-means will differ from the overall &lt;em&gt;population&lt;/em&gt; mean.
For instance, we can assume in this case that all groups might actually have a more negative attitude towards smoking after seeing the text or visual message no matter whether they were externally and internally (ideally we would of course use a pre-post test design but lets keep that for later).
Thus, we could for example assume that (having no better data on this) people are on average indifferent about smoking, therefore assuming the population mean is 50.
Now lets see what this would imply in terms of change scores for the numbers above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;change_TI &amp;lt;- mean_TI-50
change_VI &amp;lt;- mean_VI-50
change_TE &amp;lt;- mean_TE-50
change_VE &amp;lt;- mean_VE-50

barplot(c(change_TI, change_VI, change_TE, change_VE), names.arg = c(&amp;quot;TI&amp;quot;, &amp;quot;VI&amp;quot;, &amp;quot;TE&amp;quot;, &amp;quot;VE&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-06-16-power-analysis-by-data-simulation-in-r-part-iii_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Thus, this does not seem entirely plausible.
The VE group increases in their attitude towards cigarettes compared to the assumed population mean and the TI group does not change at all.
This is not what we would expect.
However, as we want to keep the rank-order of the bars the same (i.e. order of difference-magnitudes between the groups should stay the same as above) we will start with the highest bar here and adjust it to a position where it would actually make sense compared to the population mean.
Lets assume that the looking at these external pictures would actually decrease the population mean of smoking attitude by 3 points on average.
This is just a consequence of that I think an intervention of this would be rather weak as most people do not see these pictures for the first time and also might not be influences so much by looking at them to begin with.
Thus my mean of group_VE becomes 47.&lt;/p&gt;
&lt;p&gt;Next, when looking at the TI group, I think hat this group might decrease more strongly compared to the other group (it is internal), so the new mean after reading texts about yourself might be 45 in this case.&lt;/p&gt;
&lt;p&gt;The next group is the TE group which should be lower than the other two groups before. However given that it is also an external group, and I expect internal messages to work better, it is probably not much stronger than the TI group, so I will assume the mean is 43.&lt;/p&gt;
&lt;p&gt;Last, the strongest of the changes I would probably expect for the VI group so I can assume that the change might be 10 points in this case so that their mean will become 40.&lt;/p&gt;
&lt;p&gt;Let us plot these new means again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;focus &amp;lt;- rep(c(&amp;quot;internal&amp;quot;, &amp;quot;external&amp;quot;), each = 2)
media &amp;lt;- rep(c(&amp;quot;text&amp;quot;, &amp;quot;visual&amp;quot;), times = 2)
mean_TI &amp;lt;- 43
mean_VI &amp;lt;- 40
mean_TE &amp;lt;- 45
mean_VE &amp;lt;- 47

pd &amp;lt;- data.frame(score = c(mean_TI, mean_VI, mean_TE, mean_VE), focus = focus, media = media)

interaction.plot(pd$focus, pd$media, pd$score, ylim = c(0,100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-06-16-power-analysis-by-data-simulation-in-r-part-iii_files/figure-html/plot_exp2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(pd$score[pd$focus == &amp;quot;internal&amp;quot;]) != mean(pd$score[pd$focus == &amp;quot;external&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(pd$score[pd$media == &amp;quot;text&amp;quot;]) == mean(pd$score[pd$media == &amp;quot;visual&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in this new example, technically there is a difference between the two media groups on average but it is only .50 points, so I would argue that it is small enough to represent our assumption of “no” effect, as in real-life “no” effect in terms of a difference being actually 0 is rather rare anyway.&lt;/p&gt;
&lt;p&gt;Ok, so after rule-of-thumbing our way through all the different group-means, we have to consider what standard deviation we find plausible in this situation.&lt;/p&gt;
&lt;p&gt;It is plausible, that the intervention that we are administering might not directly influence the spread in each group, so we might as well assume that the spread is the same in each group.
What should we choose in this case?
Let us think about the population.
We said that people would in average be indifferent about smoking with a mean of 50 on a 100-point scale.
How big should the spread on this scale be, i.e. between which values do we assume most people to be on this scale.
It is probably likely that most people will not have a very good attitude about smoking, say 80 or higher.
The lower end is more difficult to predict in this case, there might as well be more people that have a very negative opinion.
In fact, the distribution might be skewed rather than symmetric, with more people having very negative opinions and a rather long tail.
In any case, we will use this upper-limit assumption to come up with some reasonable standard-deviation.
If we start at 50 and we want most people to be &amp;lt; 80, we can set the 2-SD bound at 80 to get a standard-deviation of 15 (80-50)/2.
Thus let us assume that each of our groups has a standard-deviation of 15 points.&lt;/p&gt;
&lt;p&gt;Thus our distributions that we would have are&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;group_TI = normal(n, 43, 15)
group_VI = normal(n, 40, 15)
group_TE = normal(n, 45, 15)
group_VE = normal(n, 47, 15)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these groups we can now run an ANOVA.
Before getting into the power-analysis part lets just have a look what would happen if we had 10,000 people per group and would use this in our ANOVA.
First we will do this by using the &lt;code&gt;aov_car&lt;/code&gt; function from the &lt;code&gt;afex&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: afex&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: lme4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 methods overwritten by &amp;#39;car&amp;#39;:
##   method                          from
##   influence.merMod                lme4
##   cooks.distance.influence.merMod lme4
##   dfbeta.influence.merMod         lme4
##   dfbetas.influence.merMod        lme4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ************
## Welcome to afex. For support visit: http://afex.singmann.science/&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## - Functions for ANOVAs: aov_car(), aov_ez(), and aov_4()
## - Methods for calculating p-values with mixed(): &amp;#39;KR&amp;#39;, &amp;#39;S&amp;#39;, &amp;#39;LRT&amp;#39;, and &amp;#39;PB&amp;#39;
## - &amp;#39;afex_aov&amp;#39; and &amp;#39;mixed&amp;#39; objects can be passed to emmeans() for follow-up tests
## - NEWS: library(&amp;#39;emmeans&amp;#39;) now needs to be called explicitly!
## - Get and set global package options with: afex_options()
## - Set orthogonal sum-to-zero contrasts globally: set_sum_contrasts()
## - For example analyses see: browseVignettes(&amp;quot;afex&amp;quot;)
## ************&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;afex&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:lme4&amp;#39;:
## 
##     lmer&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require(afex)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1e5
group_TI &amp;lt;-  rnorm(n, 43, 15)
group_VI &amp;lt;-  rnorm(n, 40, 15)
group_TE &amp;lt;-  rnorm(n, 45, 15)
group_VE &amp;lt;-  rnorm(n, 47, 15)

participant &amp;lt;- c(1:(n*4))
focus &amp;lt;- rep(c(&amp;quot;internal&amp;quot;, &amp;quot;external&amp;quot;), each = n*2)
media &amp;lt;- rep(c(&amp;quot;text&amp;quot;, &amp;quot;visual&amp;quot;), each = n, times = 2)

aov_dat &amp;lt;- data.frame(participant = participant, focus = focus, media = media, score = c(group_TI, group_VI, group_TE, group_VE))


aov_car(score ~ focus*media+ Error(participant), data = aov_dat, type = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Contrasts set to contr.sum for the following variables: focus, media&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Anova Table (Type 3 tests)
## 
## Response: score
##        Effect        df    MSE           F   ges p.value
## 1       focus 1, 399996 225.28 8811.28 ***   .02  &amp;lt;.0001
## 2       media 1, 399996 225.28  126.75 *** .0003  &amp;lt;.0001
## 3 focus:media 1, 399996 225.28 2703.27 ***  .007  &amp;lt;.0001
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;+&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that, if we collect 10,000 people per group, everything would be significant with focus having the biggest effect (look at the &lt;code&gt;ges&lt;/code&gt; column in the table for generalized eta-squared values) and with media having the smallest effect (in fact close to 0 as we wanted).
That even the main-effect for media is still significant just shows that this happens even with very small deviations from the exact null-hypothesis in these huge samples.&lt;/p&gt;
&lt;p&gt;Note that again we could rewrite this anova as a dummy-coded linear model as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
score_i = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_1X_2 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By making dummy-variables we can code this in &lt;code&gt;lm&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aov_dat$focus_dummy &amp;lt;- ifelse(aov_dat$focus == &amp;quot;internal&amp;quot;, 0 , 1)
aov_dat$media_dummy &amp;lt;- ifelse(aov_dat$media == &amp;quot;text&amp;quot;, 0 , 1)

lm_int &amp;lt;- lm(score ~ 1 + focus_dummy + media_dummy + focus_dummy:media_dummy, data = aov_dat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the estimates that we get here we see how the model is estimated:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; = &lt;code&gt;mean(group_TI)&lt;/code&gt; as both dummies are 0 for this one.
&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; = &lt;code&gt;mean(group_TE)-mean(group_TI)&lt;/code&gt; as the focus variable changes from 0 to 1 within the text-group
&lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; = &lt;code&gt;mean(group_VI)-mean(group_TI)&lt;/code&gt; as the media variable changes from 0 to 1 within the internal focus group
&lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; = &lt;code&gt;mean(group_VE)-mean(group_TE)+mean(group_TI)-mean(group_VI)&lt;/code&gt; to see what a crossover of the variables (i.e. the interaction) compares to &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we want to test whether the effect of the interaction, for instance is significant, we can conduct an ANOVA for the interaction to see how the model &lt;em&gt;with&lt;/em&gt; the interaction compares to a model &lt;em&gt;without&lt;/em&gt; the interaction, which we do in the following code-block by fitting a model that does not have the interaction in their and compare it with the &lt;code&gt;anova&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm_null &amp;lt;- lm(score ~ 1 + focus_dummy + media_dummy, data = aov_dat)
anova(lm_null, lm_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: score ~ 1 + focus_dummy + media_dummy
## Model 2: score ~ 1 + focus_dummy + media_dummy + focus_dummy:media_dummy
##   Res.Df      RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1 399997 90721027                                  
## 2 399996 90112027  1    609000 2703.3 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we find the same F-value and p-value as before, which should not be surprising considering we did the same test.
I will leave it to you which of the two methods, i.e. using the regular &lt;code&gt;aov&lt;/code&gt; functions or specifying linear models, you prefer.
An obvious advantage of the &lt;code&gt;aov&lt;/code&gt; function is that we do not have to specify nested models but it is also sort of a black-box approach, while in the linear models, we get estimated effects and can see whether the model does actually behaves as we would expect.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quick-detour-contrast-coding-and-centering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quick detour: Contrast Coding and Centering&lt;/h3&gt;
&lt;p&gt;Going back to the &lt;code&gt;aov_car&lt;/code&gt; output above, you might have noticed that the function informed us that it &lt;em&gt;set [contrasts] to contr.sum for the following variables: focus, media&lt;/em&gt;.
What does this mean?
It means that instead of using dummy-coding as we did when we created the dummy variables, it instead assigned a -1 to a level of the variable while setting a 1 to the other level.
We can check what this looks like if we set it manually to the variables in the data-set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aov_dat$media_sum &amp;lt;- factor(aov_dat$media)
contrasts(aov_dat$media_sum) &amp;lt;- contr.sum(2)
contrasts(aov_dat$media_sum)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        [,1]
## text      1
## visual   -1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aov_dat$focus_sum &amp;lt;- factor(aov_dat$focus)
contrasts(aov_dat$focus_sum) &amp;lt;- contr.sum(2)
contrasts(aov_dat$focus_sum)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]
## external    1
## internal   -1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we compare this to our dummy variables:&lt;/p&gt;
&lt;p&gt;media:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;text = 0&lt;/li&gt;
&lt;li&gt;visual = 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;focus:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;internal = 0&lt;/li&gt;
&lt;li&gt;external = 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s re-run the linear model with these variables and see what we get.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aov_dat$media_sum_num &amp;lt;- ifelse(aov_dat$media == &amp;quot;text&amp;quot;, 1, -1)
aov_dat$focus_sum_num &amp;lt;- ifelse(aov_dat$focus == &amp;quot;external&amp;quot;, 1, -1)

lm_int_sum &amp;lt;- lm(score ~ 1 + focus_sum_num + media_sum_num + focus_sum_num:media_sum_num, data = aov_dat)
lm_int_sum&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = score ~ 1 + focus_sum_num + media_sum_num + focus_sum_num:media_sum_num, 
##     data = aov_dat)
## 
## Coefficients:
##                 (Intercept)                focus_sum_num  
##                     43.7756                       2.2277  
##               media_sum_num  focus_sum_num:media_sum_num  
##                      0.2672                      -1.2339&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm_null_sum &amp;lt;- lm(score ~ 1 + focus_sum_num + media_sum_num, data = aov_dat)
lm_null_sum&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = score ~ 1 + focus_sum_num + media_sum_num, data = aov_dat)
## 
## Coefficients:
##   (Intercept)  focus_sum_num  media_sum_num  
##       43.7756         2.2277         0.2672&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(lm_null_sum,lm_int_sum)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: score ~ 1 + focus_sum_num + media_sum_num
## Model 2: score ~ 1 + focus_sum_num + media_sum_num + focus_sum_num:media_sum_num
##   Res.Df      RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1 399997 90721027                                  
## 2 399996 90112027  1    609000 2703.3 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see immediately that the estimates changed somewhat.
Now, for example the intercept represents the mean of all 4 other means (i.e. the grand mean):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mean(c(mean(group_TI), mean(group_TE), mean(group_VE), mean(group_VI)))&lt;/code&gt; = 43.775609.&lt;/p&gt;
&lt;p&gt;and the first estimate represents the difference of each focus level from that grand-mean, for the “internal groups this is”&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mean(c(mean(group_TI), mean(group_VI)))-mean(c(mean(group_TI), mean(group_TE), mean(group_VE), mean(group_VI)))&lt;/code&gt; = -2.2276811&lt;/p&gt;
&lt;p&gt;and for the external groups this is&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mean(c(mean(group_TE), mean(group_VE)))-mean(c(mean(group_TI), mean(group_TE), mean(group_VE), mean(group_VI)))&lt;/code&gt; = 2.2276811&lt;/p&gt;
&lt;p&gt;respectively.&lt;/p&gt;
&lt;p&gt;Why would we use this coding-scheme in the linear model instead of the other one?
The p-value for the interaction is exactly the same in both cases.
However, if we compare the p-values of main-effects here, we will see how they differ.&lt;/p&gt;
&lt;p&gt;For instance, if we tell &lt;code&gt;aov_car&lt;/code&gt; that we want to keep our dummy-coding instead of changing it, we can see how the two outputs differ.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aov_car(score ~ focus*media+ Error(participant), data = aov_dat, type = 3, afex_options(&amp;quot;check_contrasts&amp;quot; = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Anova Table (Type 3 tests)
## 
## Response: score
##        Effect        df    MSE           F  ges p.value
## 1       focus 1, 399996 225.28  876.77 *** .002  &amp;lt;.0001
## 2       media 1, 399996 225.28  829.66 *** .002  &amp;lt;.0001
## 3 focus:media 1, 399996 225.28 2703.27 *** .007  &amp;lt;.0001
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;+&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, these results are a bit weird, as we seem to have the same effect-size for both main-effects, but did we not simulate a much bigger effect of focus compared to media&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;?
Something seems to be off here.
This has to do with the fact that in classical dummy-coding (also called treatment coding) you cannot easily test main-effects when interactions are present in the model.
For the interaction on the other hand it does not matter.
You can read more about this in &lt;a href=&#34;http://talklab.psy.gla.ac.uk/tvw/catpred/#why-does-it-matter&#34;&gt;this blog post by Dale Barr&lt;/a&gt; but the take-home message here is that, whenever we have an interaction in our model, we want to use sum-to-zero coding rather than dummy-coding.
This is why the results of the last anova do not show the behavior that we simulated.&lt;/p&gt;
&lt;p&gt;Thus, for the rest of the tutorial we will use sum-to-zero contrasts coding.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-the-power-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Running the power-analysis&lt;/h3&gt;
&lt;p&gt;I will use the &lt;code&gt;lm&lt;/code&gt; way here as it is much faster (about 8 times faster in a quick test) than the &lt;code&gt;aov_car&lt;/code&gt; method.
To track our power, we can directly extract p-values from the &lt;code&gt;anova&lt;/code&gt; object, just as we did for the &lt;code&gt;t.test&lt;/code&gt; objects earlier.&lt;/p&gt;
&lt;p&gt;The only thing that we still need to do is specify our test criteria.
Imagine that this time we are hired by the government and are asked to evaluate which of the health-warnings on cigarette packages would work best in which of the situations that we investigate.
They gave us a lot of money to do this, but they do not want to draw any conclusions that are unwarranted as implementing new laws costs a lot of money.
Thus, we want to set our alpha-level quite conservatively at &lt;span class=&#34;math inline&#34;&gt;\(\alpha = .001\)&lt;/span&gt; to only make a non-warranted claim about the interaction effect in 1 of every 1,000 experiments.
Moreover, we also want to be sure that we actually detect an effect and therefore want to keep our power high at 95% to be sure that if there &lt;em&gt;is&lt;/em&gt; an interaction we would detect it in 19 out of 20 cases.&lt;/p&gt;
&lt;p&gt;Now we have everything that we need to set up the code-chunk below to run a power-analysis similar to the ones we have been running previously, just this time with an ANOVA coded as a linear model.
If we would again try out every sample-size, this would take a very long time until we reach our power criterion and we would probably still be sitting here tomorrow or the day after.
In these cases, it makes sense to run a two-stage power analysis in which we, in the first stage, try out various sample-sizes with a very low resolution (e.g. n = 10, n = 100, n = 200) etc. to get a rough idea of where we can expect our loop to end.
Afterwards, we will increase the resolution to find how many participants exactly we would need.
The first step with the low resolution is performed below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims &amp;lt;- 1000 # we want 1000 simulations
p_vals &amp;lt;- c()
power_at_n &amp;lt;- c(0) # this vector will contain the power for each sample-size (it needs the initial 0 for the while-loop to work)
n &amp;lt;- 100 # sample-size and start at 100 as we can be pretty sure this will not suffice for such a small effect
n_increase &amp;lt;- 100 # by which stepsize should n be increased
i &amp;lt;- 2

power_crit &amp;lt;- .95
alpha &amp;lt;- .001

while(power_at_n[i-1] &amp;lt; power_crit){
  for(sim in 1:n_sims){
    group_TI &amp;lt;-  rnorm(n, 43, 15)
    group_VI &amp;lt;-  rnorm(n, 40, 15)
    group_TE &amp;lt;-  rnorm(n, 45, 15)
    group_VE &amp;lt;-  rnorm(n, 47, 15)
    
    participant &amp;lt;- c(1:(n*4))
    focus &amp;lt;- rep(c(&amp;quot;internal&amp;quot;, &amp;quot;external&amp;quot;), each = n*2)
    media &amp;lt;- rep(c(&amp;quot;text&amp;quot;, &amp;quot;visual&amp;quot;), each = n, times = 2)
    
    aov_dat &amp;lt;- data.frame(participant = participant, focus = focus, media = media, score = c(group_TI, group_VI, group_TE, group_VE))
    aov_dat$media_sum_num &amp;lt;- ifelse(aov_dat$media == &amp;quot;text&amp;quot;, 1, -1) # apply sum-to-zero coding
    aov_dat$focus_sum_num &amp;lt;- ifelse(aov_dat$focus == &amp;quot;external&amp;quot;, 1, -1) 
    lm_int &amp;lt;- lm(score ~ 1 + focus_sum_num + media_sum_num + focus_sum_num:media_sum_num, data = aov_dat) # fit the model with the interaction
    lm_null &amp;lt;- lm(score ~ 1 + focus_sum_num + media_sum_num, data = aov_dat) # fit the model without the interaction
    p_vals[sim] &amp;lt;- anova(lm_int, lm_null)$`Pr(&amp;gt;F)`[2] # put the p-values in a list
  }
    print(n)
    power_at_n[i] &amp;lt;- mean(p_vals &amp;lt; alpha) # check power (i.e. proportion of p-values that are smaller than alpha-level of .10)
    names(power_at_n)[i] &amp;lt;- n
    n &amp;lt;- n+n_increase # increase sample-size by 100 for low-resolution testing first
    i &amp;lt;- i+1 # increase index of the while-loop by 1 to save power and cohens d to vector
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 100
## [1] 200
## [1] 300
## [1] 400
## [1] 500
## [1] 600
## [1] 700
## [1] 800
## [1] 900&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power_at_n &amp;lt;- power_at_n[-1] # delete first 0 from the vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(as.numeric(names(power_at_n)), power_at_n, xlab = &amp;quot;Number of participants per group&amp;quot;, ylab = &amp;quot;Power&amp;quot;, ylim = c(0,1), axes = TRUE)
abline(h = .95, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-06-16-power-analysis-by-data-simulation-in-r-part-iii_files/figure-html/plot_power_low_res-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that at roughly 900 participants our loop ran into a situation where we have sufficient power.
Thank god we did not do this 1-by-1.
Now we can have a closer look at sample-sizes between 800 and 900.
Probably, at this sample-size it does not really matter whether we calculate +/- 10 participants in terms of budget, so we can start at 800 and increase in steps of 10 and instead increase the simulation-number per sample-size to 10,000 to get even more precise results for each particular sample-size, as this is a rather important power-analysis impact-wise.
Afterwards, if we &lt;em&gt;really&lt;/em&gt; wanted to we could of course run a third round between the last two 10-people cutoffs and increase the sample-size by 1 to reach an exact sample-size.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims &amp;lt;- 10000 # we want 10000 simulations
p_vals &amp;lt;- c()
power_at_n &amp;lt;- c(0) # this vector will contain the power for each sample-size (it needs the initial 0 for the while-loop to work)
delta_r2s&amp;lt;- c()
delta_r2s_at_n &amp;lt;- c(0)
n &amp;lt;- 800 # sample-size and start at 800 as we can be pretty sure this will not suffice for such a small effect
n_increase &amp;lt;- 10 # by which stepsize should n be increased
i &amp;lt;- 2

power_crit &amp;lt;- .95
alpha &amp;lt;- .001

while(power_at_n[i-1] &amp;lt; power_crit){
  for(sim in 1:n_sims){
    group_TI &amp;lt;-  rnorm(n, 43, 15)
    group_VI &amp;lt;-  rnorm(n, 40, 15)
    group_TE &amp;lt;-  rnorm(n, 45, 15)
    group_VE &amp;lt;-  rnorm(n, 47, 15)
    
    participant &amp;lt;- c(1:(n*4))
    focus &amp;lt;- rep(c(&amp;quot;internal&amp;quot;, &amp;quot;external&amp;quot;), each = n*2)
    media &amp;lt;- rep(c(&amp;quot;text&amp;quot;, &amp;quot;visual&amp;quot;), each = n, times = 2)
    
    aov_dat &amp;lt;- data.frame(participant = participant, focus = focus, media = media, score = c(group_TI, group_VI, group_TE, group_VE))
    aov_dat$media_sum_num &amp;lt;- ifelse(aov_dat$media == &amp;quot;text&amp;quot;, 1, -1)
    aov_dat$focus_sum_num &amp;lt;- ifelse(aov_dat$focus == &amp;quot;external&amp;quot;, 1, -1)
    lm_int &amp;lt;- lm(score ~ 1 + focus_sum_num + media_sum_num + focus_sum_num:media_sum_num, data = aov_dat)
    lm_null &amp;lt;- lm(score ~ 1 + focus_sum_num + media_sum_num, data = aov_dat)
    p_vals[sim] &amp;lt;- anova(lm_null, lm_int)$`Pr(&amp;gt;F)`[2]
    delta_r2s[sim]&amp;lt;- summary(lm_int)$adj.r.squared-summary(lm_null)$adj.r.squared
    }
    print(n)
    power_at_n[i] &amp;lt;- mean(p_vals &amp;lt; alpha) # check power (i.e. proportion of p-values that are smaller than alpha-level of .10)
    delta_r2s_at_n[i] &amp;lt;- mean(delta_r2s)
    names(power_at_n)[i] &amp;lt;- n
    names(delta_r2s_at_n)[i] &amp;lt;- n
    n &amp;lt;- n+n_increase # increase sample-size by 100 for low-resolution testing first
    i &amp;lt;- i+1 # increase index of the while-loop by 1 to save power and cohens d to vector
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 800
## [1] 810
## [1] 820
## [1] 830
## [1] 840
## [1] 850
## [1] 860
## [1] 870
## [1] 880
## [1] 890&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power_at_n &amp;lt;- power_at_n[-1] # delete first 0 from the vector
delta_r2s_at_n &amp;lt;- delta_r2s_at_n[-1] # delete first 0 from the vector&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you will notice when you run this yourself, this simulation already takes quite some time.
Later, when we work with mixed models in part IV, we will see how we can run such a simulation on multiple processor-cores simultaneously to solve this problem to at least some extent.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(as.numeric(names(power_at_n)), power_at_n, xlab = &amp;quot;Number of participants per group&amp;quot;, ylab = &amp;quot;Power&amp;quot;, ylim = c(0,1), axes = TRUE)
abline(h = .95, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-06-16-power-analysis-by-data-simulation-in-r-part-iii_files/figure-html/plot_power-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This power-curve looks a bit odd, as it looks almost flat.
What this shows however, is that power is not increasing linearly but tends to increase faster at the beginning and slower towards reaching 1.&lt;/p&gt;
&lt;p&gt;In any case, 890 people &lt;em&gt;per group&lt;/em&gt; would be enough to reach our desired power of .95.
Note how close this is to the 900 from our low-resolution simulation above, showing the merits of this approach.
Also note that the change in amount of variance explained &lt;span class=&#34;math inline&#34;&gt;\(\Delta R^2\)&lt;/span&gt; only increases by about &lt;code&gt;delta_r2s_at_n[length(delta_r2s_at_n)]&lt;/code&gt; = 0.0067977.
Whether this ~0.6% is a meaningful amount of added variance explained to begin with and whether it is worth spending money in order to test about 3,500 people without even knowing what this attitude measure in this case even tells us in terms of smoking-behavior is a whole other question and cannot be answered with power-analysis (probably the answer would be that it is not so important though).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-a-numeric-predictor&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding a Numeric Predictor&lt;/h2&gt;
&lt;p&gt;In this last bit of part III of this tutorial I want to demonstrate how we can add numeric predictors to our models as, so far, we only worked with factors.
Again, knowing how to re-specify an ANOVA as a linear model is helpful in this case if we have both, factorial and numerical predictors.&lt;/p&gt;
&lt;p&gt;We could also assume that the effect that our predictors have depend on other factors.
For example, we could think that the more cigarettes a person smokes each day, the less effective any treatment would be independent of what strategy we would use.
Thus, if we assume that the aforementioned effect-sizes would be the expected value for a non-smoking population, including smokers would make this effect less pronounced.
In other words, the more a person smokes, the smaller the expected effect size for that person should be.&lt;/p&gt;
&lt;p&gt;How do we do this?
If we reformulate our problem above we could say that if a person smokes 0 cigarettes a day she should get 100% of the expected effect (e.g. the decrease of 7 points in the internal text-group).
Now, we can take the maximum amount of cigarettes that we think a person can smoke per day (or what we expect the 95% CI to be in a smoking population for instance).
Let us say we assume this to be 36 cigarettes a day (about 2 packages).
Now if we want a person that smokes 0 cigarettes to get the full 7 points and a person who smokes 36 cigarettes to get 0 percent of the effect, we can rewrite the effect like this&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y{_i} =  \bar y \times \frac{max_{n}-n}{max_n}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or if we fill in the numbers for a non-smoker:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y{_i} = 7 \times \frac{36-0}{36} = 7\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and someone who smokes 14 cigarettes a day:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y{_i} = 7 \times \frac{36-14}{36} = 5.64\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus, our effect of 7 points now depends on how much someone smokes.&lt;/p&gt;
&lt;p&gt;Now we have to come up with an assumption of how much people smoke in the population.
A quick &lt;a href=&#34;https://www.trimbos.nl/docs/9a7f5384-36fa-4edc-815f-1d0388960f46.pdf&#34;&gt;google search&lt;/a&gt; can tell us that, in the Netherlands, 23.1% of adults smoke.
We can also find that people who smoke consume ca. 11 cigarettes on average.
Thus, we will make a population in which 76.9 percent of the people do not smoke and 23.1 percent smokes 11 cigarettes on average.
For the cigarette count in the smoking population we will choose a &lt;em&gt;Poisson&lt;/em&gt; distribution, as it is often a good representation of count-data like this.&lt;/p&gt;
&lt;div class=&#34;notebox&#34;&gt;
&lt;p&gt;
You might never have heard of a Poisson distribution and to avoid confusion here: We could just use a uniform distribution between 1 and 36 for example so that smoking 30 cigarettes a day is just as likely as smoking 1 cigarette, given that a person is a smoker.
However, I do not think that this is the case and that the likelihood of a smoker smoking 30 cigarettes is smaller compared to only smoking 5 cigarettes.
The Poisson distribution mainly carries this assumption forward into the simulation.
As you continue simulating stuff in the future, you will probably get to know more of these useful distributions that neatly describe certain assumptions that we have about the data.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To create a population like this we can do the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1000# sample size

smoke_yes_no &amp;lt;- ifelse(runif(n, 0, 100) &amp;lt; 77, 0, 1) # vector telling us whether each &amp;quot;person&amp;quot; smokes or not

n_smoke &amp;lt;- c() # vector that will contain number of cigarettes per day smoked by each person

# create the number of smoked cigarettes
for(i in 1:length(smoke_yes_no)){ 
  if(smoke_yes_no[i] == 1){
    n_smoke[i] &amp;lt;- rpois(1,11) # simulate 1 cigarette count from a poisson distribution with mean 11
  } else{ 
    n_smoke[i] &amp;lt;- 0}
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code we check for each person whether they are a smoker or not and if so, say that the person’s number of cigarettes smoked comes from a Poisson distribution or otherwise it will be 0 by definition.&lt;/p&gt;
&lt;p&gt;To implement this for each group, we can just replace the means of the groups by this formula:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
group_TI &amp;lt;-  rnorm(n, (50-(7*((max(n_smoke)-n_smoke)/max(n_smoke)))), 15)
group_VI &amp;lt;-  rnorm(n, (50-(10*((max(n_smoke)-n_smoke)/max(n_smoke)))), 15)
group_TE &amp;lt;-  rnorm(n, (50-(5*((max(n_smoke)-n_smoke)/max(n_smoke)))), 15)
group_VE &amp;lt;-  rnorm(n, (50-(3*((max(n_smoke)-n_smoke)/max(n_smoke)))), 15)

participant &amp;lt;- c(1:(n*4))
focus &amp;lt;- rep(c(&amp;quot;internal&amp;quot;, &amp;quot;external&amp;quot;), each = n*2)
media &amp;lt;- rep(c(&amp;quot;text&amp;quot;, &amp;quot;visual&amp;quot;), each = n, times = 2)

aov_dat &amp;lt;- data.frame(participant = participant, focus = focus, media = media, score = c(group_TI, group_VI, group_TE, group_VE))

aov_dat$media_sum_num &amp;lt;- ifelse(aov_dat$media == &amp;quot;text&amp;quot;, 1, -1)
aov_dat$focus_sum_num &amp;lt;- ifelse(aov_dat$focus == &amp;quot;external&amp;quot;, 1, -1)
aov_dat$n_smoke &amp;lt;- rep(scale(n_smoke))

lm_int &amp;lt;- lm(score ~ 1 + focus_sum_num * media_sum_num * n_smoke, data = aov_dat)
summary(lm_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = score ~ 1 + focus_sum_num * media_sum_num * n_smoke, 
##     data = aov_dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -50.287 -10.129   0.189   9.930  48.593 
## 
## Coefficients:
##                                     Estimate Std. Error t value
## (Intercept)                         44.44569    0.23621 188.160
## focus_sum_num                        2.12720    0.23621   9.005
## media_sum_num                        0.21372    0.23621   0.905
## n_smoke                              1.77929    0.23633   7.529
## focus_sum_num:media_sum_num         -0.81755    0.23621  -3.461
## focus_sum_num:n_smoke               -0.27972    0.23633  -1.184
## media_sum_num:n_smoke               -0.10452    0.23633  -0.442
## focus_sum_num:media_sum_num:n_smoke  0.06719    0.23633   0.284
##                                               Pr(&amp;gt;|t|)    
## (Intercept)                                    &amp;lt; 2e-16 ***
## focus_sum_num                                  &amp;lt; 2e-16 ***
## media_sum_num                                 0.365634    
## n_smoke                             0.0000000000000629 ***
## focus_sum_num:media_sum_num                   0.000544 ***
## focus_sum_num:n_smoke                         0.236651    
## media_sum_num:n_smoke                         0.658334    
## focus_sum_num:media_sum_num:n_smoke           0.776180    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 14.94 on 3992 degrees of freedom
## Multiple R-squared:  0.03674,    Adjusted R-squared:  0.03505 
## F-statistic: 21.75 on 7 and 3992 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how the effects that we put in before are still there but they are weaker now as we have smokers included in our sample.
Moreover, smoking causes a more positive attitude towards smoking, just as we wanted, and the interaction between media and focus is not influenced by whether someone smokes or not, as we put the same punishment for the treatment effect in all the groups, leaving the interaction uninfluenced by whether someone smokes or not.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;So now we have seen how we can simulate data for analyses that many people use in their everyday life as a (psychological) researcher.
So far, all of the power-analyses that we did can also be done in standard software like G*Power (though in part I of this tutorial I explained why I still prefer simulation).
However, as nice as these analyses are, we often deal with more complicated situations where we have to deal with hierarchical or cross-classified data.
In these cases, other approaches like linear-mixed effects models, also called hierarchical models, need to be used to analyse our data.
This is where simulation really shines as the structure of the data strongly depends on the research design and in a point-and-click software (though existing) it is difficult to offer all the flexibility that we might need.
Thus, in the fourth part, we will see how we can simulate data and run a power-analysis in these cases.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;footnotes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Footnotes&lt;/h1&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;We can actually see that this is the case by calculating cohens d for both assuming that we would perform a t-test for each of the 2 factors with the between-subject cohens d formula from part II of this tutorial:
&lt;strong&gt;focus&lt;/strong&gt;:
&lt;code&gt;(mean(aov_dat$score[aov_dat$focus == &#34;internal&#34;])-mean(aov_dat$score[aov_dat$focus == &#34;external&#34;]))/sqrt((sd(aov_dat$score[aov_dat$focus == &#34;internal&#34;])^2+sd(aov_dat$score[aov_dat$focus == &#34;external&#34;])^2)/2)&lt;/code&gt; = -0.2824849
&lt;strong&gt;media&lt;/strong&gt;:
&lt;code&gt;(mean(aov_dat$score[aov_dat$media == &#34;text&#34;])-mean(aov_dat$score[aov_dat$media == &#34;visual&#34;]))/sqrt((sd(aov_dat$score[aov_dat$media == &#34;text&#34;])^2+sd(aov_dat$score[aov_dat$media == &#34;visual&#34;])^2)/2)&lt;/code&gt; = 0.0281052&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Power Analysis by Data Simulation in R - Part II</title>
      <link>https://julianquandt.com/post/power-analysis-by-data-simulation-in-r-part-ii/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      <guid>https://julianquandt.com/post/power-analysis-by-data-simulation-in-r-part-ii/</guid>
      <description>
&lt;script src=&#34;https://julianquandt.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-power-analysis-by-simulation-in-r-for-really-any-design---part-ii&#34;&gt;The Power Analysis by simulation in &lt;code&gt;R&lt;/code&gt; for really any design - Part II&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#simulating-a-between-subjects-t-test&#34;&gt;Simulating a between-subjects t-test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simulating-a-within-subject-t-test&#34;&gt;Simulating a within-subject t-test&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#using-a-one-sample-t-test-approach&#34;&gt;Using a one-sample t-test approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-a-correlated-samples-paired-t-test-approach&#34;&gt;Using a correlated-samples paired t-test approach&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-our-first-simulations-with-t-tests&#34;&gt;Summary: Our first simulations with t-tests&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#footnotes&#34;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;html&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;https://julianquandt.com/css/style.css&#34; /&gt;
&lt;/html&gt;
&lt;div class=&#34;notebox&#34;&gt;
&lt;p&gt;
&lt;p&gt;&lt;strong&gt;Click &lt;a href=&#34;https://julianquandt.com/rmd_files/2020-05-12-power-analysis-by-data-simulation-in-r-part-ii.Rmd%22&#34;&gt;HERE&lt;/a&gt; to download the .Rmd file&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This blog is also available on &lt;a href=&#34;https://www.r-bloggers.com/&#34;&gt;R-Bloggers&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;style type=&#34;text/css&#34;&gt;
button.btn.collapsed:before
{
    content:&#39;+&#39; ;
    display:block;
    width:15px;
}
button.btn:before
{
    content:&#39;-&#39; ;
    display:block;
    width:15px;
}
&lt;/style&gt;
&lt;div id=&#34;the-power-analysis-by-simulation-in-r-for-really-any-design---part-ii&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Power Analysis by simulation in &lt;code&gt;R&lt;/code&gt; for really any design - Part II&lt;/h1&gt;
&lt;p&gt;This is Part II of my tutorial on how to do power-analysis by simulation.
In Part I, we saw how to do a simulation for a simple toy-example with a coin-toss.
In this part, we will use a more realistic problem that we might encounter in our daily research life and see how to simulate the power for these designs.
By looking at how to do power-simulation for the independent-samples t-test and the paired t-test we will learn how to simulate normal-distributions, how to specify their effect-sizes, in terms of &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt;. Moreover, we simulate correlated (i.e. multivariate) normal distributions in cases where we have correlated observations (e.g. paired-sample t-test).
This will be an important tool for later parts of this tutorial.&lt;/p&gt;
&lt;p&gt;In part III of this tutorial we will learn how we can conceptualize basically &lt;em&gt;any&lt;/em&gt; design as a linear model and thereby be very flexible in our power analysis.
In part IV we will learn how to apply this technique to complicated designs such as linear mixed-effects models and generalized mixed-effects models.&lt;/p&gt;
&lt;div id=&#34;simulating-a-between-subjects-t-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating a between-subjects t-test&lt;/h2&gt;
&lt;p&gt;Let’s get to it.
The first thing we will need again in our simulation is one of the implemented simulation functions in R (those that let &lt;code&gt;R&lt;/code&gt; run theoretical experiments for us), but this time it is not &lt;code&gt;rbinom&lt;/code&gt; as we are not working with coin-flips but &lt;code&gt;rnorm&lt;/code&gt; - the simulation function for the normal distribution.
Let’s have a short look at that function as we will keep working with it throughout the tutorial.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;rnorm(n, mean, sd)&lt;/code&gt; takes three arguments, a sample-size &lt;code&gt;n&lt;/code&gt;, a &lt;code&gt;mean&lt;/code&gt; and a standard-deviation &lt;code&gt;sd&lt;/code&gt;.
By specifying these values, we can sample random ‘people’ (or observations) that are participating in our simulated experiments.
Imagine, for example, that we have an intervention study in which we have a treatment group and a control group.
We can easily simulate both groups with &lt;code&gt;rnorm&lt;/code&gt; but what should the means and sds of the groups be?&lt;/p&gt;
&lt;p&gt;There are two ways we can approach this.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We could think about what group means we expect in our given case and what we expect the spread of the groups to be &lt;em&gt;on the measurment scale that we are working with&lt;/em&gt;.
For example, if we use a 40-point scale for a clinical test we might know that a group with deficiencies on the thing that we measure would probably score around 10 points and that almost everyone from that group would score lower than 20 points.
This statement (most people score around 10, almost everyone scores lower than tified as normal distribution with a mean of 10 and a standard-deviation of 5. In this case only 2.5% of the values (i.e. the values outside the 95% CI) will be higher than 20.&lt;/li&gt;
&lt;li&gt;In a new research project, we might not be able or willing to to make these statements.
In this case, by making some extra assumptions, we can fall back to the approach that we also use in power-calculation software in most cases and define a &lt;em&gt;standardized effect size&lt;/em&gt; that we can use to simulate data rather than defining the group means and standard-deviations directly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I personally try to go with the first approach whenever possible, as I think that in many cases we know more about what we expect from our data than we think, even in new projects.
Even if we &lt;strong&gt;do not&lt;/strong&gt; know a lot about our data, we might still try out different assumptions (i.e. means and sds) for the groups in our simulation to see what power we would get for each of them.
This way, we can make informed decisions about our sample size that are more nuanced than the one in which we just assume a standardized effect size and see what sample-size it implies and are forced to think harder about our data - something that might seem difficult and annoying at first, but is extremely useful and eduucational.
Another advantage of specifying the groups directly is that we can do this for any arbitrarily complex design where standardized effect sizes are often difficult to calculate.&lt;/p&gt;
&lt;p&gt;This said, for the cases where we might really not be willing to specify groups directly, and because it allows me to demonstrate some other interesting points, in this part I will discuss how we can use standardized effect-sizes in our simulation.
In part III and IV however, we will always specify effects on the raw scale.&lt;/p&gt;
&lt;p&gt;If we were using GPower now, we would most likely just fill in a difference between groups in &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; and be done with it.
We could of course also follow this approac in a simulation by defining the groups based on the implied &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt;.
For instance, we can just assume that group 1 as &lt;code&gt;rnorm(n, 1,2)&lt;/code&gt;.
Now, following from the formula for Cohen’s d:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Cohen&amp;#39;s\ d =  \frac{(M_1 - M_2)}{pooled \ sd}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[pooled\ sd =  \sqrt\frac{(sd_1^2+sd_2^2)}{2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and adhering to the student t-test assumption of equal variances we can fill in the pooled sd formula above as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[pooled\ sd =  \sqrt\frac{(2^2+2^2)}{2} = 2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to get a &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; of .50:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Cohen&amp;#39;s\ d =  \frac{(1 - 0)}{2} = 0.5\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To get any other value for &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; we can just change the pooled sd value to whatever we want.
More generally, we want to solve the equation above for the pooled sd after specifying any &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt;, e.g.:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[0.5=  \frac{(1 - 0)}{pooled\ sd}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can solve an equation like that with &lt;code&gt;R&lt;/code&gt;’s somewhat unintuitive &lt;code&gt;solve&lt;/code&gt; function like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solve(0.5,1) # cohens d of .5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solve(0.25,1) # cohens d of .25&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solve(2,1) # cohens d of 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;giving us three examples of how we would need to specify pooled sd to arrive at a particular &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Thus, if we want to do a t-test with two simulated groups and a cohen’s d of 0.5 we can simulate two groups of a particular sample-size by using the &lt;code&gt;rnorm&lt;/code&gt; function.
Let’s say we have 30 participants in each group.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
group1 &amp;lt;- rnorm(30, 1, 2)
group2 &amp;lt;- rnorm(30, 0, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the groups that we got in a plot like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(group1, col = &amp;quot;#addd8e&amp;quot;, breaks = 10, main = &amp;quot;Histogram of both groups&amp;quot;, xlab = &amp;quot;&amp;quot;)
hist(group2, add = TRUE, breaks = 10, col= &amp;quot;#31a354&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-05-12-power-analysis-by-data-simulation-in-r-part-ii_files/figure-html/plot_groups1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can already make important observations from this plot:&lt;/p&gt;
&lt;p&gt;We wanted to get normal distributions, but what we got here does not really look normal.
Why is that? Because we only have 30 people per group and taking only 30 values from the specified normal distributions does not really give us a good approximation of the real distribution.
&lt;strong&gt;This point is important&lt;/strong&gt;: The sampling variability in such small groups is high and often, if small sample-studies (i.e. underpowered studies) find “effects”, they are often rather big and the consequence of this sampling variability rather than real differences of groups.
For example, by looking at the means of our sampled groups &lt;code&gt;mean(group1)&lt;/code&gt; = 0.40715 and &lt;code&gt;mean(group2)&lt;/code&gt; = -1.1032366 we see that the group mean of group 1 is actually closer to the mean that we specified for group 2 (i.e. 0) than to its own mean, while the mean for group 2 is far away from our intended mean.
Looking at the sds actually shows that they are quite close to what we wanted &lt;code&gt;sd(group1)&lt;/code&gt; = 1.8059661 and &lt;code&gt;sd(group2)&lt;/code&gt; = 1.9179992.
The &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; that we wanted is also not presented very accurately at &lt;code&gt;(mean(group1)-mean(group2))/(sqrt((sd(group1)^2+sd(group2)^2)/2))&lt;/code&gt; = 0.8108043.
Again, if we would do this in Gpower, and specify a &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt;, we will always work with an &lt;em&gt;exact&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt;, in a simulation approach we do &lt;strong&gt;not&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So let us run a t-test to see whether there is a significant difference here.
First, we need to decide on an alpha-level again.
What will we choose?
Well, to have a good justification we have to elaborate on what the groups actually represent.
Let us say that the difference between groups is related to an intervention that can elevate depressive symptoms.
Thus, the control group (group1) did not get the intervention and scores higher on depressive symptoms while the treatment group (group2) is expected to score lower.
Let us assume that this is the first study that we run and that, if we find anything we will follow it up by more extensive studies anyway. Therefore, we might not want to miss a possible effect by setting a too conservative alpha-level.
If we find something in this study, we will conduct further studies in which we are more strict about the alpha level.
Thus, we choose .10 for this first “pilot” study.&lt;/p&gt;
&lt;div class=&#34;notebox&#34;&gt;
&lt;p&gt;
&lt;strong&gt;NOTE&lt;/strong&gt;: The alpha-level “jusficications” in this tutorial are for educational purposes and to provide a starting point. They are obviously not as rigorous as we would like in a real research project. If you find yourself in a situation where you want to justify your alpha-level see &lt;a href=&#34;https://www.nature.com/articles/s41562-018-0311-x&#34;&gt;Justify your alpha by Lakens et al.&lt;/a&gt; for a good discussion on this.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We can now run a t-test with R’s integrated &lt;code&gt;t.test&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(group1, group2, paired = FALSE, var.equal = TRUE, conf.level = 0.9)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Two Sample t-test
## 
## data:  group1 and group2
## t = 3.1402, df = 58, p-value = 0.002656
## alternative hypothesis: true difference in means is not equal to 0
## 90 percent confidence interval:
##  0.7064042 2.3143690
## sample estimates:
## mean of x mean of y 
##  0.407150 -1.103237&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The t-test shows, that this effect would be significant.
However, we also got “lucky” and had a larger effect than we intended to have.
To do a proper power analysis (lets say we first want to see whether 30 people per group are enough) we need to not only simulate each group once, but many many times and see how often we get a significant result at the desired alpha-level&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.
Moreover, we would like to have a power of at least 95%, again reflecting our view that we do not want to miss a possible effect.&lt;/p&gt;
&lt;p&gt;In normal language these assumptions mean that if there is a difference, we will detect it in 19 out of 20 cases while, if there is no difference, we will only be incorrectly claiming that there is one in 1 out of 10 cases.&lt;/p&gt;
&lt;p&gt;We will do this similarly to our simulations in part 1 of this tutorial.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims &amp;lt;- 1000 # we want 1000 simulations
p_vals &amp;lt;- c()
for(i in 1:n_sims){
  group1 &amp;lt;- rnorm(30,1,2) # simulate group 1
  group2 &amp;lt;- rnorm(30,0,2) # simulate group 2
  p_vals[i] &amp;lt;- t.test(group1, group2, paired = FALSE, var.equal = TRUE, conf.level = 0.90)$p.value # run t-test and extract the p-value
}
mean(p_vals &amp;lt; .10) # check power (i.e. proportion of p-values that are smaller than alpha-level of .10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.592&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Aha, so it appears that our power &lt;code&gt;mean(p_vals &amp;lt; .10)&lt;/code&gt; = 0.592 is much lower than the 95% that we desired.
Thus, we did really get lucky in our example above when we found an effect of our intervention.&lt;/p&gt;
&lt;p&gt;To actually do a legit power-analysis however, we would like to know how many people we do need for a power of 95 percent.
Again we can modify the code above to take this into account.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims &amp;lt;- 1000 # we want 1000 simulations
p_vals &amp;lt;- c()
power_at_n &amp;lt;- c(0) # this vector will contain the power for each sample-size (it needs the initial 0 for the while-loop to work)
cohens_ds &amp;lt;- c()
cohens_ds_at_n &amp;lt;- c() 
n &amp;lt;- 30 # sample-size 
i &amp;lt;- 2
while(power_at_n[i-1] &amp;lt; .95){
  for(sim in 1:n_sims){
    group1 &amp;lt;- rnorm(n,1,2) # simulate group 1
    group2 &amp;lt;- rnorm(n,0,2) # simulate group 2
    p_vals[sim] &amp;lt;- t.test(group1, group2, paired = FALSE, var.equal = TRUE, conf.level = 0.9)$p.value # run t-test and extract the p-value
    cohens_ds[sim] &amp;lt;- abs((mean(group1)-mean(group2))/(sqrt((sd(group1)^2+sd(group2)^2)/2))) # we also save the cohens ds that we observed in each simulation
  }
    power_at_n[i] &amp;lt;- mean(p_vals &amp;lt; .10) # check power (i.e. proportion of p-values that are smaller than alpha-level of .10)
    cohens_ds_at_n[i] &amp;lt;- mean(cohens_ds) # calculate means of cohens ds for each sample-size
    n &amp;lt;- n+1 # increase sample-size by 1
    i &amp;lt;- i+1 # increase index of the while-loop by 1 to save power and cohens d to vector
}
power_at_n &amp;lt;- power_at_n[-1] # delete first 0 from the vector
cohens_ds_at_n &amp;lt;- cohens_ds_at_n[-1] # delete first NA from the vector&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The loop stopped at a sample-size of &lt;code&gt;n-1&lt;/code&gt; = 84 participants per group.
Thus make a conclusion about the effectiveness of our intervention at the specified alpha-level with the desired power we need 168 people in total.&lt;/p&gt;
&lt;p&gt;To visualize the power we can plot it again, just as in the first part of the tutorial.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(30:(n-1), power_at_n, xlab = &amp;quot;Number of participants per group&amp;quot;, ylab = &amp;quot;Power&amp;quot;, ylim = c(0,1), axes = TRUE)
abline(h = .95, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-05-12-power-analysis-by-data-simulation-in-r-part-ii_files/figure-html/plot_power1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, this plot shows us how our power to detect the effect slowly increases if we increase the sample-size until it reaches our desired power.&lt;/p&gt;
&lt;p&gt;There is another interesting observation to make here.
In the code above, I also calculate the average &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; for each sample size and the plot below shows how it changes with increasing sample-size.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(30:(n-1), cohens_ds_at_n, xlab = &amp;quot;Number of participants per group&amp;quot;, ylab = &amp;quot;Cohens D&amp;quot;, ylim = c(0.45,0.55), axes = TRUE)
abline(h = .50, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-05-12-power-analysis-by-data-simulation-in-r-part-ii_files/figure-html/plot_cohensd1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is not super obvious in this plot and I had to change the scale of the y-axis quite a bit to make it visible, but we can actually see how our average &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; initially deviates slightly more from the desired &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; of .50 than in de end.
In other words, in the beginning, for small sample-sizes there is more fluctuation than for bigger sample-sizes.
That is pretty neat, as it seems very desirable that a power-estimation procedure takes into account that for smaller sample-sizes, even if the effect in the population is exactly the same (i.e. we always sample groups with a difference of &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; = .50) it is just less precise.&lt;/p&gt;
&lt;p&gt;Let’s have a brief summary of what we did so far.
We just used the formula for &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; to give our groups a certain difference that we are interested in, ran 1000 simulated experiments for each sample-size and calculated the power, just as in the first part of the tutorial.&lt;/p&gt;
&lt;p&gt;However, I want to mention again that, even though it is convenient to specify the effect-size this way as it saves us from having to specify precise group means and standard-deviations directy and makes the specification more comparable, it is often preferable to specify the parameters on the original scale that we are interested in.
This is especially the case if we have previous data on a research topic that we can make use of.
Moreover, for more complex designs with many parameters, standardized effect sizes are often difficult to obtain and we are forced to make our assumptions on the original scale of the data.
We will see this in later examples.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-a-within-subject-t-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating a within-subject t-test&lt;/h2&gt;
&lt;p&gt;Intuitively, it might seem that we can use the exact same approach above for a paired t-test as well.
However, the problem with this is that in a paired t-test we get 2 data-points from the same individual.
For example, image we have a group of people that get an intervention and we measure their score before and after the intervention and want to compare them with a paired t-test.
In this case, the score of the post-measure of a given individual is not completely independent of the score of the pre-measure.
In other words, somebody who scores very low on the pre-measure will most likely not score very high on the post-measure and vice versa.&lt;/p&gt;
&lt;p&gt;Thus, there is a &lt;em&gt;correlation&lt;/em&gt; between the pre- and the post-measures in that the pre-measures already tell us a little bit about what we can expect on the post-measure.
You probably already knew this but why does this matter for power simulation, you might wonder.
It matters as it directly influences our power to detect an effect as we will see later.
For now let’s just keep in mind that it is important.&lt;/p&gt;
&lt;p&gt;So what do we do in a situation with correlated data as in the pre-post intervention situation?
There are two ways we can go from here.
First, we can simulate correlated normal distributions, as already mentioned above.
However, for the particular case of a paired sample t-test, we can also just make use of the fact that, in the end, we are testing whether the &lt;strong&gt;difference&lt;/strong&gt; between post- and pre-measures is different from 0.
In this case, the correlation between the pre and the post-measure is implicitely handled when substracting the two measures. This way, we do not need to directly specify it.
If the correlation is close to one, the standard-deviation of the difference scores will be very small, if it is zero, we will end up with the same situation that we have in the independent-sample t-test.
Thus, we can just make use of a one-sample in which we test whether the distribution of difference-scores differs from zero as the paired t-test is equivalent to the one-sample t-test on difference scores &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3840331/&#34;&gt;(see Lakens, 2013 for more details on this)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Though the one-sample approach is easier to simulate, I will describe both approaches in the following as the first approach (simulating correlated normal-distributions) is more flexible and we need it for the situations we deal with later.&lt;/p&gt;
&lt;div id=&#34;using-a-one-sample-t-test-approach&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using a one-sample t-test approach&lt;/h3&gt;
&lt;p&gt;When we want to do our power-calculation based on the one-sample t-test approach, we only have to specify a single difference-score distribution.
We can do this again, based on the &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; formula, this time for a one-sample scenario:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Cohen&amp;#39;s\ d = \frac{M_{diff} - \mu_0}{SD_{diff}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the above formula, to get our values for the simulation we can substitute the &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; by 0 (as our null-hypothesis is no difference) and solve the equation in the same way as above by fixing the mean-difference between pre- and post-measure, &lt;span class=&#34;math inline&#34;&gt;\(M_{diff}\)&lt;/span&gt; to 1 and calculating the sd we need for each given &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt;, for instance&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 0.5 = \frac{1}{SD_{diff}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;putting this into &lt;code&gt;R&lt;/code&gt;s &lt;code&gt;solve&lt;/code&gt; function again, we unsurprisingly get a 2 in this case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solve(0.5, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run our simulation we just need to modify the code above to run a one-sample t-test rather than a two-sample t-test and change the formula for &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims &amp;lt;- 1000 # we want 1000 simulations
p_vals &amp;lt;- c()
power_at_n &amp;lt;- c(0) # this vector will contain the power for each sample-size (it needs the initial 0 for the while-loop to work)
cohens_ds &amp;lt;- c()
cohens_ds_at_n &amp;lt;- c() 
n &amp;lt;- 2 # sample-size 
i &amp;lt;- 2
while(power_at_n[i-1] &amp;lt; .95){
  for(sim in 1:n_sims){
    difference &amp;lt;- rnorm(n,1,2) # simulate the difference score distribution
    p_vals[sim] &amp;lt;- t.test(difference, mu = 0, conf.level = 0.90)$p.value # run t-test and extract the p-value
    cohens_ds[sim] &amp;lt;- mean(difference)/sd(difference) # we also save the cohens ds that we observed in each simulation 
  }
    power_at_n[i] &amp;lt;- mean(p_vals &amp;lt; .10) # check power (i.e. proportion of p-values that are smaller than alpha-level of .10)
    cohens_ds_at_n[i] &amp;lt;- mean(cohens_ds) # calculate means of cohens ds for each sample-size
    n &amp;lt;- n+1 # increase sample-size by 1
    i &amp;lt;- i+1 # increase index of the while-loop by 1 to save power and cohens d to vector
}
power_at_n &amp;lt;- power_at_n[-1] # delete first 0 from the vector
cohens_ds_at_n &amp;lt;- cohens_ds_at_n[-1] # delete first NA from the vector&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the loop stopped at &lt;code&gt;n&lt;/code&gt; = 43 so the sample size we need is &lt;code&gt;n-1&lt;/code&gt; = 42&lt;/p&gt;
&lt;p&gt;We can plot the power-curve again&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(2:(n-1), power_at_n, xlab = &amp;quot;Number of participants per group&amp;quot;, ylab = &amp;quot;Power&amp;quot;, ylim = c(0,1), axes = TRUE)
abline(h = .95, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-05-12-power-analysis-by-data-simulation-in-r-part-ii_files/figure-html/plot_power_ost-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and the &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(2:(n-1), cohens_ds_at_n, xlab = &amp;quot;Number of participants per group&amp;quot;, ylab = &amp;quot;Cohens D&amp;quot;, ylim = c(0.0,1.0), axes = TRUE)
abline(h = .50, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-05-12-power-analysis-by-data-simulation-in-r-part-ii_files/figure-html/plot_cohensd_ost-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see again, and this time more dramatically, how our simulated effect size becomes more accurate the bigger our sample gets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-a-correlated-samples-paired-t-test-approach&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using a correlated-samples paired t-test approach&lt;/h3&gt;
&lt;div id=&#34;the-relationship-between-sd_diff-and-the-correlation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The relationship between &lt;span class=&#34;math inline&#34;&gt;\(SD_{diff}\)&lt;/span&gt; and the correlation&lt;/h4&gt;
&lt;p&gt;In the above example, we respecified a paired t-test as a one-sample t-test on the difference scores.
However, what we are actually working with is two &lt;em&gt;correlated&lt;/em&gt; distributions of measurements.
To demonstrate this point, let us have a look at how we would actually calculate the standard deviation of the difference scores (&lt;span class=&#34;math inline&#34;&gt;\(SD_{diff}\)&lt;/span&gt;) in the above equation for &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt;.
The formula to calculate &lt;span class=&#34;math inline&#34;&gt;\(SD_{diff}\)&lt;/span&gt; from the standard deviation of the two measurements (pre and post) and their correlation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SD_{diff} = \sqrt{SD_{pre}^2+SD_{post}^2-2r \times SD_{pre} \times SD_{post}} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is not important at this point to understand why this is the case (we will just trust &lt;a href=&#34;http://www.utstat.toronto.edu/~brunner/oldclass/378f16/readings/CohenPower.pdf&#34;&gt;Cohen&lt;/a&gt; on this) but see how we can, for any given &lt;span class=&#34;math inline&#34;&gt;\(SD_{diff}\)&lt;/span&gt; and assuming both groups have, for example, a standard deviation of 2, solve the formula to see what correlation (the &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; in the above formula) it would imply.&lt;/p&gt;
&lt;p&gt;Imagine, for instance, we assume (like in the independent-samples t-test above) that both measures have a standard-deviation of 2 and that the standard-deviation of the difference scores would also be 2 so that we would have the same situation as in the one-sample t-test example above, where we had a mean-difference of 1 and a difference-score standard-deviation of 2.&lt;/p&gt;
&lt;p&gt;Filling this in we get&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[2 = \sqrt{2^2+2^2-2r \times 2 \times 2} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Solving this equation for &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, we get&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;span class=&#34;math inline&#34;&gt;\(r = 0.5\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;Therefore, interestingly the situation in which we use the same groups as above and assume that we would get the same &lt;span class=&#34;math inline&#34;&gt;\(SD_{diff}\)&lt;/span&gt; of 2 as we assumed in our one-sample situation would imply that the correlation between the pre- and the post-measure is &lt;span class=&#34;math inline&#34;&gt;\(r = 0.5\)&lt;/span&gt;.
What does this mean?
Well, lets see what happens if we assume a correlation of &lt;span class=&#34;math inline&#34;&gt;\(r = .90\)&lt;/span&gt; and see what &lt;span class=&#34;math inline&#34;&gt;\(SD_{diff}\)&lt;/span&gt; we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SD_{diff} = \sqrt{2^2+2^2-2 \times 0.90 \times 2 \times 2} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Solving this in &lt;code&gt;R&lt;/code&gt; gives us: &lt;code&gt;sqrt(2^2+2^2-2*0.9*2*2)&lt;/code&gt; = 0.89.
Thus, if the correlation increases the standard-deviation of the difference-scores becomes smaller.
If we do the same with a correlation of .10 we get &lt;code&gt;sqrt(2^2+2^2-2*0.1*2*2)&lt;/code&gt; = 2.68.
Thus, when the correlation decreases the standard-deviation becomes bigger.
Interestingly, this demonstrates that for the same mean-difference, a high correlation results in a &lt;em&gt;larger&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; as calculated for the difference scores in the one-sample case.
In other words, as the pre-scores tend to be more similar to the post-scores (i.e. they have a high correlation), the standard-deviation of the difference scores decreases.
This, in turn, results in higher power to detect an effect.&lt;/p&gt;
&lt;p&gt;To sum up all of the above, we can either specify a difference-score distribution directly and thereby imply a certain correlation by specifying the &lt;span class=&#34;math inline&#34;&gt;\(SD_{diff}\)&lt;/span&gt;, or we can see what &lt;span class=&#34;math inline&#34;&gt;\(SD_{diff}\)&lt;/span&gt; we get with a certain correlation by using the formula above and use the result for the one-sample simulation.
However, instead of working with the one-sample t-test, in the next section, we will see how we can directly simulate correlated normal-distributions in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-correlated-normal-distributions-and-demystifying-the-multivariate-normal.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Simulating correlated normal-distributions and demystifying the multivariate normal.&lt;/h4&gt;
&lt;p&gt;In real life, almost everything is correlated to some degree.
Though these correlations are often not of interest, they sometimes are and in a good simulation we want to acknowledge them. For instance, predictors in a regression might be correlated or random effects in a mixed-model.&lt;/p&gt;
&lt;p&gt;The following part is (again) longer than I intended but I feel that it is important to understand how we simulate correlated normal-distributions and what a multivariate normal-distribution is.
In most cases later on we will deal with some kind of correlated normal distributions (in mixed-models we will always encounter them for example) so I think it helps if we have a look at them now in an easier example, so we have one problem less to worry about later on.&lt;/p&gt;
&lt;p&gt;Rephrasing the problem of simulating two correlated normal-distributions, we can say that we want to simulate a &lt;em&gt;multivariate normal distribution&lt;/em&gt; or, more specifically in this case, a &lt;em&gt;bivariate normal distribution&lt;/em&gt;.
If you never heard these terms before, they might seem very opague, so let’s see what they are.
I will first show how we can simulate them, and explain what exactly this multivariate normal distribution means afterwards with a little visual intuiton.
We can simulate a multivariate normal distribution by using the &lt;code&gt;mvrnorm()&lt;/code&gt; function from the &lt;code&gt;MASS&lt;/code&gt; package but it works slightly different than the simulation functions that we have used so far (&lt;code&gt;rnorm&lt;/code&gt; and &lt;code&gt;rbinom&lt;/code&gt;).
Lets have a look at how this works (code explained below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require(MASS) # load MASS package&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: MASS&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pre_post_means &amp;lt;- c(pre = 0,post = 1) # define means of pre and post in a vector
pre_sd &amp;lt;- 2 # define sd of pre-measure
post_sd &amp;lt;- 2 # define sd of post-measure
correlation &amp;lt;- 0.5 # define their correlation

sigma &amp;lt;- matrix(c(pre_sd^2, pre_sd*post_sd*correlation, pre_sd*post_sd*correlation, post_sd^2), ncol = 2) # define variance-covariance matrix

set.seed(1)
bivnorm &amp;lt;- data.frame(mvrnorm(10000, pre_post_means, sigma)) # simulate bivariate normal&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code samples 10,000 observations from a bivariate normal-distribution, or in terms of our example, it samples 10,000 pre-measures with 10,000 correlated post-measures.
The first thing that is different from our earlier simulations is the first line of the code &lt;code&gt;pre_post_means &amp;lt;- c(0,1)&lt;/code&gt;.
Instead of defining our means seperately for each measurement, as we have done earlier in the independent-sample case, we now put the pre- and post-measurement mean that we assume into a vector.
This is because we will simulate both measurements together in the &lt;code&gt;mvrnorm&lt;/code&gt; function, and therefore both means need to be provided at the same time.&lt;/p&gt;
&lt;p&gt;Secondly, we define the standard-deviations of both measurements just as we did earlier and also specify a correlation that we would like our data-points to have, in this case 0.5.&lt;/p&gt;
&lt;p&gt;Now, the line &lt;code&gt;matrix(c(pre_sd^2, pre_sd*post_sd*correlation, pre_sd*post_sd*correlation, post_sd^2), ncol = 2)&lt;/code&gt; does something that we have not done before and it might look quite confusing.
What we are doing here is specifying the &lt;em&gt;variance-covariance matrix&lt;/em&gt;.
This is nothing more than a table containing the variances of our pre- and post-measurement (the first and the last entry in the list) and the covariance between the two variables twice - once for each measurement (the middle 2 entries in the list).
Conceptually you can see this variance-covariance matrix as the standard-deviation of the multivariate normal that &lt;code&gt;mvrnorm&lt;/code&gt; needs instead of the standard-deviation that we put into &lt;code&gt;rnorm&lt;/code&gt; earlier.&lt;/p&gt;
&lt;p&gt;We can visualize the variance-covariance matrix &lt;code&gt;sigma&lt;/code&gt; to demystify it a bit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colnames(sigma) &amp;lt;- c(&amp;quot;pre&amp;quot;, &amp;quot;post&amp;quot;)
rownames(sigma) &amp;lt;- c(&amp;quot;pre&amp;quot;, &amp;quot;post&amp;quot;)
sigma&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      pre post
## pre    4    2
## post   2    4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, this matrix is nothing more than a table containing the variance of each variable (4 in each case) and their covariance (i.e. the correlation of the two multiplied by both standard-deviations (&lt;span class=&#34;math inline&#34;&gt;\(Cov(pre,post) = \rho(pre,post)*sd_{pre}*sd_{post}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;In the next line of the code we put this all into &lt;code&gt;mvrnorm&lt;/code&gt; to simulate our bivariate normal distribution and store the results in a data-frame with 2 columns, each containing one measurement point:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(bivnorm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          pre       post
## 1 -0.2807182 -0.8893814
## 2  1.3746052  0.2615539
## 3 -0.4119554 -1.4827470
## 4  3.9486678  2.5775470
## 5  1.0711637  1.0702847
## 6 -0.8961042 -0.9460816&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we run &lt;code&gt;cor(bivnorm$pre, bivnorm$post)&lt;/code&gt; we see that indeed their correlation is 0.52 and close to what we specified.&lt;/p&gt;
&lt;p&gt;To see how we can imagine such a bivariate normal distribution, we can visualize it the following way.&lt;/p&gt;
&lt;p&gt;If we draw a histogram of each measurement individually, it looks like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,2))
hist(bivnorm$pre, main = &amp;quot;pre-measure&amp;quot;)
hist(bivnorm$post, main = &amp;quot;post-measure&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-05-12-power-analysis-by-data-simulation-in-r-part-ii_files/figure-html/plot_bivnorm1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, imagine we would not only look at each histogram seperately but we would combine them into one plot by putting the pre-measure scores of each simulated individual on the x-axis and putting the post-measures on the y-axis like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(bivnorm$pre, bivnorm$post, xlab = &amp;quot;pre-measure&amp;quot;, ylab = &amp;quot;post-measure&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-05-12-power-analysis-by-data-simulation-in-r-part-ii_files/figure-html/plot_bivnorm2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the above plot, we can clearly see the correlations between the two measurements that we put in the data.
More elegantly, we can combine the two histograms in the following way.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bivnorm_kde &amp;lt;- kde2d(bivnorm[,1], bivnorm[,2], n = 50) # calculate kernel density (i.e. the &amp;quot;height of the cone on the z-axis&amp;quot;; not so important to understand here)
par(mar = c(0, 0, 0, 0)) # tel r not to leave so much space around the plot
persp(bivnorm_kde, phi = 45, theta = 30, xlab = &amp;quot;pre-measure&amp;quot;, ylab = &amp;quot;post-measure&amp;quot;, zlab = &amp;quot;frequency&amp;quot;) # plot the bivariate normal&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-05-12-power-analysis-by-data-simulation-in-r-part-ii_files/figure-html/plot_bivnorm3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, we see clearly how our bivariate normal distribution is nothing more than the 2 normal-distributions of each measurement-point combined into one “cone-shaped” normal distribution that has a certain correlation.&lt;/p&gt;
&lt;p&gt;The plot below shows how this cone looks with different correlations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-05-12-power-analysis-by-data-simulation-in-r-part-ii_files/figure-html/plot_bivnorm_cors-1.png&#34; width=&#34;120%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how for higher correlations, the cone becomes more and more narrow and starts looking like a “shark-fin” with a correlation of .90.
This “narrowring” of the cone is the visualization of why the standard-deviations of the difference scores get more narrow.&lt;/p&gt;
&lt;p&gt;If we visualize this as a point cloud again the three correlations look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bivnorm_10 &amp;lt;- as.data.frame(bivnorm_10)
bivnorm_90 &amp;lt;- as.data.frame(bivnorm_90)
par(mfrow = c(1,3))
plot(bivnorm_10$pre, bivnorm_10$post)
plot(bivnorm$pre, bivnorm$post)
plot(bivnorm_90$pre, bivnorm_90$post)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-05-12-power-analysis-by-data-simulation-in-r-part-ii_files/figure-html/correlation_pointplots-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This again, clearly shows the manipulatino between the pre- and post measures.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;power-analysis-with-the-multivariate-normal&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Power-analysis with the multivariate normal&lt;/h4&gt;
&lt;p&gt;Now that we know what we are doing when using &lt;code&gt;mvrnorm&lt;/code&gt; we can go ahead and do a power-simulation for the example above with a bivariate normal-distribution.
However, as we are not sure how big our correlation is, we can try 3 different correlations in the code above by placing the simulation in another for-loop and telling it to try different correlations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu_pre_post &amp;lt;- c(pre = 0, post = 1)
sd_pre &amp;lt;- 2
sd_post &amp;lt;- 2
correlations &amp;lt;- c(0.1, 0.5, 0.9)

set.seed(1)
n_sims &amp;lt;- 1000 # we want 1000 simulations
p_vals &amp;lt;- c()
# this vector will contain the power for each sample-size (it needs the initial 0 for the while-loop to work)
cohens_ds &amp;lt;- c()

powers_at_cor &amp;lt;- list()
cohens_ds_at_cor &amp;lt;- list()

for(icor in 1:length(correlations)){ # do a power-simulation for each specified simulation
  n &amp;lt;- 2 # sample-size 
  i &amp;lt;- 2 # index of the while loop for saving things into the right place in the lists
  power_at_n &amp;lt;- c(0) 
  cohens_ds_at_n &amp;lt;- c() 
  sigma &amp;lt;- matrix(c(sd_pre^2, sd_pre*sd_post*correlations[icor], sd_pre*sd_post*correlations[icor], sd_post^2), ncol = 2) #var-covar matrix
  while(power_at_n[i-1] &amp;lt; .95){
    for(sim in 1:n_sims){
      bivnorm &amp;lt;- data.frame(mvrnorm(n, mu_pre_post, sigma)) # simulate the bivariate normal
      p_vals[sim] &amp;lt;- t.test(bivnorm$pre, bivnorm$post, paired = TRUE, var.equal = TRUE, conf.level = 0.9)$p.value # run t-test and extract the p-value
      cohens_ds[sim] &amp;lt;- abs((mean(bivnorm$pre)-mean(bivnorm$post))/(sqrt(sd(bivnorm$pre)^2+sd(bivnorm$post)^2-2*cor(bivnorm$pre, bivnorm$post)*sd(bivnorm$pre)*sd(bivnorm$post)))) # we also save the cohens ds that we observed in each simulation
    }
    power_at_n[i] &amp;lt;- mean(p_vals &amp;lt; .10) # check power (i.e. proportion of p-values that are smaller than alpha-level of .10)
    names(power_at_n)[i] &amp;lt;- n
    cohens_ds_at_n[i] &amp;lt;- mean(cohens_ds) # calculate means of cohens ds for each sample-size
    names(cohens_ds_at_n)[i] &amp;lt;- n
    n &amp;lt;- n+1 # increase sample-size by 1
    i &amp;lt;- i+1 # increase index of the while-loop by 1 to save power and cohens d to vector
  }
  power_at_n &amp;lt;- power_at_n[-1] # delete first 0 from the vector
  cohens_ds_at_n &amp;lt;- cohens_ds_at_n[-1] # delete first NA from the vector
  powers_at_cor[[icor]] &amp;lt;- power_at_n # store the entire power curve for this correlation in a list
  cohens_ds_at_cor[[icor]] &amp;lt;- cohens_ds_at_n # do the same for cohens d
  names(powers_at_cor)[[icor]] &amp;lt;- correlations[icor] # name the power-curve in the list according to the tested correlation
  names(cohens_ds_at_cor)[[icor]] &amp;lt;- correlations[icor] # same for cohens d
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, the above code runs a power-simulation, or more specifically three power-analyses, one for each correlation that we wanted to test.
Notice how this time we specify &lt;code&gt;paired = TRUE&lt;/code&gt; in the &lt;code&gt;t.test&lt;/code&gt; function, to indicate that we are dealing with non-independent observations.
Also note that a new part of the code saves the &lt;code&gt;power_at_n&lt;/code&gt; vector to a list called &lt;code&gt;power_at_cor&lt;/code&gt;.
This list, will have 3 elements, each of them the power curve for one of the correlations.
We can access each power-curve bei either &lt;code&gt;powers_at_cor[[1]]&lt;/code&gt; to get the first vector in the list (the double square brackets mean first entire vector rather than first number only) or we can use it by indicating its name as &lt;code&gt;powers_at_cor$&lt;/code&gt;0.1` to tell R that we want the power curve for a correlation of .10.&lt;/p&gt;
&lt;p&gt;We can plot these power-curves next to each other&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,3))
plot(2:(length(powers_at_cor$`0.1`)+1), powers_at_cor$`0.1`, xlab = &amp;quot;Number of participants&amp;quot;, ylab = &amp;quot;Power&amp;quot;, ylim = c(0,1), axes = TRUE, main = &amp;quot;correlation = 0.1&amp;quot;)
abline(h = .95, col = &amp;quot;red&amp;quot;)
plot(2:(length(powers_at_cor$`0.5`)+1), powers_at_cor$`0.5`, xlab = &amp;quot;Number of participants&amp;quot;, ylab = &amp;quot;Power&amp;quot;, ylim = c(0,1), axes = TRUE, main = &amp;quot;correlation = 0.5&amp;quot;)
abline(h = .95, col = &amp;quot;red&amp;quot;)
plot(2:(length(powers_at_cor$`0.9`)+1), powers_at_cor$`0.9`, xlab = &amp;quot;Number of participants&amp;quot;, ylab = &amp;quot;Power&amp;quot;, ylim = c(0,1), axes = TRUE, main = &amp;quot;correlation = 0.9&amp;quot;)
abline(h = .95, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-05-12-power-analysis-by-data-simulation-in-r-part-ii_files/figure-html/powercurve_corr_t-test-1.png&#34; width=&#34;120%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we see how drastically the correlation influences the power in this situation.
With a high correlation, we need only very few participants to achieve the desired power in the specified case.
Why is this?
The reason for this is what we had a look at above: The decreasing standard-deviation of the difference scores the higher the correlation gets.&lt;/p&gt;
&lt;p&gt;This is how the effect-sizes look.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,3))
plot(2:(length(cohens_ds_at_cor$`0.1`)+1), cohens_ds_at_cor$`0.1`, xlab = &amp;quot;Number of participants&amp;quot;, ylab = &amp;quot;Cohens D&amp;quot;, ylim = c(0,1), axes = TRUE, main = &amp;quot;correlation = 0.1&amp;quot;)
abline(h = .50, col = &amp;quot;red&amp;quot;)
plot(2:(length(cohens_ds_at_cor$`0.5`)+1), cohens_ds_at_cor$`0.5`, xlab = &amp;quot;Number of participants&amp;quot;, ylab = &amp;quot;Cohens D&amp;quot;, ylim = c(0,1), axes = TRUE, main = &amp;quot;correlation = 0.5&amp;quot;)
abline(h = .50, col = &amp;quot;red&amp;quot;)
plot(2:(length(cohens_ds_at_cor$`0.9`)+1), cohens_ds_at_cor$`0.9`, xlab = &amp;quot;Number of participants&amp;quot;, ylab = &amp;quot;Cohens D&amp;quot;, ylim = c(0,10), axes = TRUE, main = &amp;quot;correlation = 0.9&amp;quot;)
abline(h = .50, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-05-12-power-analysis-by-data-simulation-in-r-part-ii_files/figure-html/cohens_d_corr_ttest-1.png&#34; width=&#34;120%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For .10 the value of the effect-size seems slightly underestimated, for .50 it approaches .50 just as in the two-sample case and for .90 it seems overestimated by quite a bit.
Did something go wrong?
Well no. As we’ve seen above &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; is calculated by dividing the mean-difference by the standard-deviation of the difference scores which becomes smaller and smaller with increasing correlation.
Therefore, calculated this way, &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; is much bigger in the case with the larger correlation.
This is also why we seem to have much bigger power - we just work with a larger effect size than we intended.
We can even calculate by how much the effect-size is influenced by the correlation by dividing the effect-size that we would calculate based on the means and sds of our groups by &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{2(1-r)}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;r = .90 –&amp;gt; &lt;code&gt;0.5/sqrt(2*(1-.90))&lt;/code&gt; = 1.118034
r = .50 –&amp;gt; &lt;code&gt;0.5/sqrt(2*(1-.50))&lt;/code&gt; = 0.5
r = .10 –&amp;gt; &lt;code&gt;0.5/sqrt(2*(1-.10))&lt;/code&gt; = 0.372678&lt;/p&gt;
&lt;p&gt;You might wonder how we can specify effect-sizes in these cases of correlated data. Do we “correct” the expected effect for the correlation or do we just assume that it is .50 and use the one-sample scenario above?
I do not have a good answer for this.
In many cases it might be fine to only specify the effect-size of the pre-post design based on the difference scores as we did in the one-sample case.
In some cases, however, we might find the correlation very important or have more information about the correlation of 2 measures than about the change in measures due to an intervention.
In those cases, it might make sense to be very specific about the expected correlations and be aware that we might need more data if the correlation is low.
Eventually, our data stem from an underlying &lt;em&gt;data generating process&lt;/em&gt; that includes the correlation between variables and measures and it is always good to be aware of the factors that might possibly influence the results.
When we collect data in a pre-post design, we &lt;em&gt;do&lt;/em&gt; in fact measure a score at 2 time-points and do not directly assess the difference.
When we specify the standard-deviation of the difference scores however, to arrive at a given &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt;, we implicitely make assumptions about the correlations of these two measures.&lt;/p&gt;
&lt;p&gt;The Take-home message here is that correlations matter and that we need to be aware of this. The good news is that power-simulations will at least make us aware of these factors and show us how different assumptions lead to different results.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-our-first-simulations-with-t-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary: Our first simulations with t-tests&lt;/h2&gt;
&lt;p&gt;This was the last bit that I wanted to discuss about simulating t-tests and the end of part II of this tutorial.
We have now learned how to simulate a t-test by using either &lt;span class=&#34;math inline&#34;&gt;\(Cohen&amp;#39;s\ d\)&lt;/span&gt; as an effect-size estimate and, if necessary, tell &lt;code&gt;R&lt;/code&gt; that our two groups, or measurements, are correlated in some way.
What we learned above is not restricted to doing t-tests however.
Simulating univariate (i.e. uncorrelated) or multivariate (i.e. correlated) normal-distributions will be what we do most of the time in part III and part IV of the tutorial.
The only thing that will change for more complicated designs is how we combine the different tools that we learned in this part to achieve our goal.&lt;/p&gt;
&lt;p&gt;In part III of this tutorial, we will see how we can basically run every analysis as a linear model using the &lt;code&gt;lm&lt;/code&gt; function instead of using the &lt;code&gt;t.test&lt;/code&gt; function for t-tests, the &lt;code&gt;aov&lt;/code&gt; function for ANOVA-designs and so forth.
By exploring how this works for t-test, anova and regression we will simulate our way through the third part and be flexible enough to simulate any classical research designs that we would, for example, be able to do in GPower. In part IV we will go beyond this and simulate mixed-effect models.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;footnotes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Footnotes&lt;/h1&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Think back to the possible sequences of coin tosses in part I.
Instead of possible sequences of coin-tosses, we deal with possible sequences of people-scores here, assuming that they come from the underlying distribution that we specify.
To get a good approximation of all the possible samples that we could get that still follow the specified distribution, we need to simulate many, many times.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;This is how we solve for r:&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;
&lt;mtable displaystyle=&#34;true&#34; columnalign=&#34;right left right left right left right left right left right left&#34; columnspacing=&#34;0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em&#34; rowspacing=&#34;3pt&#34;&gt;
&lt;mtr&gt;
&lt;mtd&gt;&lt;/mtd&gt;
&lt;mtd&gt;
&lt;mn&gt;2&lt;/mn&gt;
&lt;mo&gt;=&lt;/mo&gt;
&lt;msqrt&gt;
&lt;msup&gt;
&lt;mn&gt;2&lt;/mn&gt;
&lt;mn&gt;2&lt;/mn&gt;
&lt;/msup&gt;
&lt;mo&gt;+&lt;/mo&gt;
&lt;msup&gt;
&lt;mn&gt;2&lt;/mn&gt;
&lt;mn&gt;2&lt;/mn&gt;
&lt;/msup&gt;
&lt;mo&gt;−&lt;/mo&gt;
&lt;mn&gt;2&lt;/mn&gt;
&lt;mi&gt;r&lt;/mi&gt;
&lt;mo&gt;×&lt;/mo&gt;
&lt;mn&gt;2&lt;/mn&gt;
&lt;mo&gt;×&lt;/mo&gt;
&lt;mn&gt;2&lt;/mn&gt;
&lt;/msqrt&gt;
&lt;/mtd&gt;
&lt;/mtr&gt;
&lt;mtr&gt;
&lt;mtd&gt;
&lt;mstyle scriptlevel=&#34;0&#34;&gt;
&lt;mspace width=&#34;thickmathspace&#34;&gt;&lt;/mspace&gt;
&lt;/mstyle&gt;
&lt;mo stretchy=&#34;false&#34;&gt;⟺&lt;/mo&gt;
&lt;mstyle scriptlevel=&#34;0&#34;&gt;
&lt;mspace width=&#34;thickmathspace&#34;&gt;&lt;/mspace&gt;
&lt;/mstyle&gt;
&lt;/mtd&gt;
&lt;mtd&gt;
&lt;mn&gt;2&lt;/mn&gt;
&lt;mo&gt;=&lt;/mo&gt;
&lt;msqrt&gt;
&lt;mn&gt;8&lt;/mn&gt;
&lt;mo&gt;−&lt;/mo&gt;
&lt;mn&gt;8&lt;/mn&gt;
&lt;mi&gt;r&lt;/mi&gt;
&lt;/msqrt&gt;
&lt;/mtd&gt;
&lt;mtd&gt;
&lt;mi&gt;&lt;/mi&gt;
&lt;msup&gt;
&lt;mo stretchy=&#34;false&#34;&gt;|&lt;/mo&gt;
&lt;mn&gt;2&lt;/mn&gt;
&lt;/msup&gt;
&lt;/mtd&gt;
&lt;/mtr&gt;
&lt;mtr&gt;
&lt;mtd&gt;
&lt;mstyle scriptlevel=&#34;0&#34;&gt;
&lt;mspace width=&#34;thickmathspace&#34;&gt;&lt;/mspace&gt;
&lt;/mstyle&gt;
&lt;mo stretchy=&#34;false&#34;&gt;⟺&lt;/mo&gt;
&lt;mstyle scriptlevel=&#34;0&#34;&gt;
&lt;mspace width=&#34;thickmathspace&#34;&gt;&lt;/mspace&gt;
&lt;/mstyle&gt;
&lt;/mtd&gt;
&lt;mtd&gt;
&lt;mn&gt;4&lt;/mn&gt;
&lt;mo&gt;=&lt;/mo&gt;
&lt;mn&gt;8&lt;/mn&gt;
&lt;mo&gt;−&lt;/mo&gt;
&lt;mn&gt;8&lt;/mn&gt;
&lt;mi&gt;r&lt;/mi&gt;
&lt;/mtd&gt;
&lt;mtd&gt;
&lt;mi&gt;&lt;/mi&gt;
&lt;mrow&gt;
&lt;mo stretchy=&#34;false&#34;&gt;|&lt;/mo&gt;
&lt;/mrow&gt;
&lt;mo&gt;−&lt;/mo&gt;
&lt;mn&gt;8&lt;/mn&gt;
&lt;mo&gt;;&lt;/mo&gt;
&lt;mo&gt;÷&lt;/mo&gt;
&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;
&lt;mo&gt;−&lt;/mo&gt;
&lt;mn&gt;8&lt;/mn&gt;
&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;
&lt;/mtd&gt;
&lt;/mtr&gt;
&lt;mtr&gt;
&lt;mtd&gt;
&lt;mstyle scriptlevel=&#34;0&#34;&gt;
&lt;mspace width=&#34;thickmathspace&#34;&gt;&lt;/mspace&gt;
&lt;/mstyle&gt;
&lt;mo stretchy=&#34;false&#34;&gt;⟺&lt;/mo&gt;
&lt;mstyle scriptlevel=&#34;0&#34;&gt;
&lt;mspace width=&#34;thickmathspace&#34;&gt;&lt;/mspace&gt;
&lt;/mstyle&gt;
&lt;/mtd&gt;
&lt;mtd&gt;
&lt;mn&gt;0.5&lt;/mn&gt;
&lt;mo&gt;=&lt;/mo&gt;
&lt;mi&gt;r&lt;/mi&gt;
&lt;/mtd&gt;
&lt;/mtr&gt;
&lt;/mtable&gt;
&lt;/math&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Power Analysis by Data Simulation in R - Part I</title>
      <link>https://julianquandt.com/post/power-analysis-by-data-simulation-in-r-part-i/</link>
      <pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://julianquandt.com/post/power-analysis-by-data-simulation-in-r-part-i/</guid>
      <description>
&lt;script src=&#34;https://julianquandt.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#power-analysis-by-data-simulation-in-r---part-i-theoretical-introduction-to-simulation&#34;&gt;Power Analysis by Data Simulation in &lt;code&gt;R&lt;/code&gt; - Part I: Theoretical introduction to simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#back-to-power&#34;&gt;Back to Power&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#finally-an-actual-power-simulation.&#34;&gt;Finally, an actual power simulation.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#footnotes&#34;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;html&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;https://julianquandt.com/css/style.css&#34; /&gt;
&lt;/html&gt;
&lt;div class=&#34;notebox&#34;&gt;
&lt;p&gt;
&lt;p&gt;&lt;strong&gt;Click &lt;a href=&#34;https://julianquandt.com/rmd_files/2020-04-22-power-analysis-by-data-simulation-in-r-part-i.Rmd%22&#34;&gt;HERE&lt;/a&gt; to download the .Rmd file&lt;/strong&gt;&lt;/p&gt;
&lt;em&gt;This blog is also available on &lt;a href=&#34;https://www.r-bloggers.com/&#34;&gt;R-Bloggers&lt;/a&gt;&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;power-analysis-by-data-simulation-in-r---part-i-theoretical-introduction-to-simulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Power Analysis by Data Simulation in &lt;code&gt;R&lt;/code&gt; - Part I: Theoretical introduction to simulation&lt;/h1&gt;
&lt;div id=&#34;why-this-blog&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why this blog?&lt;/h2&gt;
&lt;p&gt;In recent years, power-analysis has become a standard tool in the behavioral sciences.
With an ongoing replication crisis, high-power research is the a key to improving replicability and to improving the reliability of findings.
Especially with preregistration slowly becoming standard practice in psychology, power-analysis, the practice of estimating the required sample-size a-priori, is a more important step than ever to plan research projects accordingly.&lt;/p&gt;
&lt;p&gt;Not so long ago, power-analysis was a rather complicated endeavor urging people to use algebraic solutions to calculate power of planned studies which can be demanding especially for non-trivial designs (i.e. basically anything that is not a correlation).
Luckily, for many research designs power-analysis is nowadays readily available in software packages such as &lt;a href=&#34;http://www.gpower.hhu.de/&#34;&gt;G*Power&lt;/a&gt; and even for relatively complex designs in specialized tools such as the &lt;em&gt;great&lt;/em&gt; &lt;a href=&#34;https://jakewestfall.shinyapps.io/pangea/&#34;&gt;PANGEA&lt;/a&gt; tool for all kinds of generalized ANOVA designs and other tools by &lt;a href=&#34;http://jakewestfall.org/&#34;&gt;Jake Westfall&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, while these tools are really great they also have (in my personal opinion) two drawbacks.
First, they urge the user to familiarize themselves with a new piece of software with new user interfaces, that are not always intuitive.
Second, and more importantly, these interfaces promote a statistical way of thinking that often leaves the user confused with what the ever-changing parameters (think of these &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; , &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f^2\)&lt;/span&gt; and vague “group-size” and “measurement-point” fields) that need to be filled in mean.
Moreover, these parameters differ for most designs and give an impression that power-analysis is complicated business better left to statisticians or that it might not be worth the effort.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;https://julianquandt.com/img/power-tutorial/me-doing-pa.jpg&#34; alt=&#34;drawing&#34; width=&#34;500&#34;/&gt;
&lt;figcaption&gt;
&lt;em&gt;Figure 1. Me trying to figure out how to use a standard power-analysis software.&lt;/em&gt;
&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;However, this impression changed dramatically for me, once I changed my ways and started doing power analysis by simulation.
Moreover, learning how to simulate data can of course not only be of use for power analysis but is a useful skill to in every research project.
When we simulate data, we can see whether what we &lt;em&gt;think&lt;/em&gt; about the data-generating process will actually &lt;em&gt;result&lt;/em&gt; in the patterns that we would expect.
In other words, we can do theoretical experiments to see whether if everything goes as we would expect, we would also find the results that we would expect.
Especially when our analyses become more complex, all the different parts in our model tend to interact and it is easy to get lost.
By simulating data before conducting an experiment, we will be forced (and able) to specify more precisely how we think a theoretical model or prediction will be reflected in the data.&lt;/p&gt;
&lt;p&gt;Thus, even though this tutorial will focus on simulating data for power analysis, you will learn a very useful skill on the side - simulating your own data and thereby conducting theoretical experiments before you even collect data.
I know that there are already some excellent tutorials on data/power-simulation out there but they are often very brief and/or technical and assume a rather high level of prior knowledge about &lt;code&gt;R&lt;/code&gt; and data-simulation in general.
Therefore, I will spend some time on explaining theoretical concepts and slowly build up the simulation-code to hopefully enable the reader to understand the underlying principles and flexibly conduct power simulations themselves after reading this.&lt;/p&gt;
&lt;p&gt;Throughout the tutorial I assume readers are familiar with &lt;code&gt;R&lt;/code&gt; and some of it’s base functionality.
This tutorial will consist of four different parts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the first part (the one you are reading) I will give a short overview of how power-analysis by simulation works on a conceptual level and why I prefer it to available power-analysis software, even though there are clear drawbacks that I will also briefly mention.
Moreover, I will introduce the concept of power as I want to bring us all on the same page and want to give a (hopefully) intuitive example about what we actually do in power calculation and how it relates to the simulation techniques that we will use for the rest of the tutorial.&lt;/li&gt;
&lt;li&gt;In the second part, I will discuss simulations for the simplest case of paired and two-sample t-tests.&lt;/li&gt;
&lt;li&gt;In the third part, we will explore different ANOVA and regression designs.&lt;/li&gt;
&lt;li&gt;In the fourth part we will move on to more complex mixed-effects and hierarchical models and even have a peak at Bayesian approaches to power analysis (well technically its not a power analysis but a true detection rate analysis).
For this fourth part, I assume readers will be familiar with how to fit mixed-effect models in &lt;code&gt;lmer&lt;/code&gt; and/or &lt;code&gt;brms&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This part of the tutorial is by far the most wordy and longest part.
Just as a little motivator to keep in mind during this sometimes lengthy tutorial:
At the end of part IV of this tutorial, you will know how to do your own custom power analysis for mixed-effects models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;power-analysis-by-simulation-a-sustainable-alternative&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Power Analysis by Simulation (a sustainable alternative)&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&#34;https://julianquandt.com/img/power-tutorial/new-power.jpg&#34; width=&#34;500&#34;/&gt;
&lt;figcaption&gt;
&lt;em&gt;Figure 2. The proof that there are alternative ways to how we normally get our power.&lt;/em&gt;
&lt;/figcaption&gt;
&lt;figure&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-power-again-a-brief-introduction&#34; class=&#34;section level2 tabset tabset-fade tabset-pills&#34;&gt;
&lt;h2&gt;What is power again? A &lt;del&gt;brief&lt;/del&gt; introduction&lt;/h2&gt;
&lt;p&gt;As my intention is to keep this post as short as possible (SPOILER: which definitely did not work), let’s directly dive into the topic by having a look at the definition of power:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If a certain effect of interest exists (e.g. a difference between two groups) power is the chance that we actually find the effect in a given study.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To provide some intuition about power, lets assume you toss a coin 10 times and you get 10 heads.
Should you be surprised about this?
Intuitively, it makes sense that we should be more surprised the more often we toss the coin and it keeps landing on head. 10 out of 10 heads would for example be less surprising than 10,000 out of 10,000, right?
This is exactly the question we want to answer when with power analysis&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.
How often should we toss the coin until we are surprised enough to conclude that the coin is not fair.&lt;/p&gt;
&lt;p&gt;To translate the above situation into a Frequentist null-hypothesis significance testing (NHST) scenario, we hypothesize that the coin &lt;em&gt;is&lt;/em&gt; indeed fair (null-hypothesis) and see whether or not the observed number of heads (the observed data) fits with this hypothesis or not. If yes, we will retain the null-hypothesis that the coin is fair, if not, we will conclude that the data are very unlikely to result from tossing a fair coin.&lt;/p&gt;
&lt;p&gt;How do we do this?
Well, assume we observed 3 heads out of 3 total tosses.
Now we can count all ways that this could have happened with a fair coin and compare it to all possible outcomes that our coin toss experiment might have produced.
These possibilities are (for a fair coin or any coin that cannot produce only heads or only tails):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Toss #1&lt;/th&gt;
&lt;th&gt;Toss #2&lt;/th&gt;
&lt;th&gt;Toss #3&lt;/th&gt;
&lt;th&gt;number of heads&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Possibility #1&lt;/td&gt;
&lt;td&gt;HEAD&lt;/td&gt;
&lt;td&gt;HEAD&lt;/td&gt;
&lt;td&gt;HEAD&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Possibility #2&lt;/td&gt;
&lt;td&gt;HEAD&lt;/td&gt;
&lt;td&gt;HEAD&lt;/td&gt;
&lt;td&gt;TAIL&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Possibility #3&lt;/td&gt;
&lt;td&gt;HEAD&lt;/td&gt;
&lt;td&gt;TAIL&lt;/td&gt;
&lt;td&gt;TAIL&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Possibility #4&lt;/td&gt;
&lt;td&gt;TAIL&lt;/td&gt;
&lt;td&gt;TAIL&lt;/td&gt;
&lt;td&gt;TAIL&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Possibility #5&lt;/td&gt;
&lt;td&gt;TAIL&lt;/td&gt;
&lt;td&gt;TAIL&lt;/td&gt;
&lt;td&gt;HEAD&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Possibility #6&lt;/td&gt;
&lt;td&gt;TAIL&lt;/td&gt;
&lt;td&gt;HEAD&lt;/td&gt;
&lt;td&gt;HEAD&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Possibility #7&lt;/td&gt;
&lt;td&gt;TAIL&lt;/td&gt;
&lt;td&gt;HEAD&lt;/td&gt;
&lt;td&gt;TAIL&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Possibility #8&lt;/td&gt;
&lt;td&gt;HEAD&lt;/td&gt;
&lt;td&gt;TAIL&lt;/td&gt;
&lt;td&gt;HEAD&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Notice that a possibility is not only defined by the number of heads and tails but also by the order in which they occur&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. We can also calculate the number of possibilities as &lt;span class=&#34;math inline&#34;&gt;\(2^x\)&lt;/span&gt; where 2 means that we have 2 possible outcomes per toss and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the number of tosses. In this case the number of possibilities is therefore &lt;span class=&#34;math inline&#34;&gt;\(2^3 = 8\)&lt;/span&gt;.
Only 1 of these 8 events (Possibility 1) can produce 3 out of 3 heads.
If our coin is fair, each of these events should be equally likely and we can see that when flipping a fair coin 3 times, in only 1 out of 8 cases (12.5%) we will get 3 heads (Possibility 1).&lt;/p&gt;
&lt;p&gt;Do we find this surprising enough to conclude that the coin that we flipped is unfair?
Maybe, maybe not.
If not, instead of flipping the coin three times, we could flip it 100 times.
Imagine we observed 55 heads in 100 flips.
We could now start writing down all possible outcomes, but that would take some time.
This time, there are not 8 but &lt;span class=&#34;math inline&#34;&gt;\(2^{100}\)&lt;/span&gt; = 1,267,651,000,000,000,000,000,000,000,000 possible sequences of heads and tails, and we would have to count the ones that produce &lt;em&gt;at least&lt;/em&gt; 55 heads to see how often that would happen if our coin is fair&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, luckily we are not the first who are interested in these kind of problems and we can make use of mathematical formulas that other people figured out for us.&lt;/p&gt;
&lt;p&gt;In this case we need the &lt;strong&gt;binomial&lt;/strong&gt; probability mass function.
In short, this function defines how often we can get each outcome, assuming a certain chance of getting heads or tails.
If you are interested in seeing this function and see how we can hand-code it in &lt;code&gt;R&lt;/code&gt;, then click on the info-box below.
If you do not want to get too much into the technicalities here, you can also just read on.&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;div class=&#34;notebox&#34;&gt;
&lt;p&gt;
&lt;strong&gt;Click here to extend information about the binomial likelihood function&lt;/strong&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;notebox&#34;&gt;
&lt;p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(x)=\frac{N!}{x!(N-x)!}\pi^x(1-\pi)^{N-x} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For people who are not used to mathematical formulas, this might already look intimidating. However, all we need to know right now is that this formula gives us the probability of getting x heads &lt;span class=&#34;math inline&#34;&gt;\(P{(x)}\)&lt;/span&gt; (i.e. the number of ways we get x heads divided by N tosses):
Note that &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; in the above formula is not the one we might know from geometry but it is simply the Greek letter for p denoting a probability here.
In this case, it is the probability of either event, heads or tails, happening on each toss so it is 50% or 0.5.
We can fill this in for the example above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(55)=\frac{100!}{55!(100-55)!}0.5^{55}(1-0.5)^{100-55}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Translating this formula into &lt;code&gt;R&lt;/code&gt; syntax we get the following:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;factorial(100)/(factorial(55)*factorial(100-55))*0.5^55*(1-0.5)^(100-55)&lt;/code&gt; = 0.05&lt;/p&gt;
&lt;p&gt;However, before we said that we do not need the probability of &lt;em&gt;exactly&lt;/em&gt; 55 heads but everything that is &lt;em&gt;at least 55&lt;/em&gt;. In order to answer our question how often we get &lt;em&gt;at least&lt;/em&gt; 55 heads, we could repeat the above calculation with all values from 55 up until 100 and add up the probabilities that we get.&lt;/p&gt;
&lt;p&gt;For example we could do this with a for-loop:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# first we write a function that calculates the probability for each number so we can call it in a loop
pbinom2 &amp;lt;- function(N, x, p){
  factorial(N)/(factorial(x)*factorial(N-x))*p^x*(1-p)^(N-x)
}

tosses_55to100 &amp;lt;- c(55:100)  # we define the amount of heads that we want to check for (all bigger than or equal to 55)
probs &amp;lt;- c() # we will make an empty collection that we will add the results for each number of heads to

for(i in tosses_55to100){
  probs &amp;lt;- append(probs, pbinom2(100,i,.5))
}
print(probs[1:10])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.048474297 0.038952560 0.030068643 0.022292270 0.015869073 0.010843867
##  [7] 0.007110732 0.004472880 0.002697928 0.001559739&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we got all the probabilities for each of the amounts of heads that we are interested in.
By summing them up we get what we need - the probability of getting at least 55 heads in 100 tosses, &lt;code&gt;sum(probs)&lt;/code&gt; = 0.18 or 18 percent.&lt;/p&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;As mentioned, fortunately, the counting of possibilities is pre-implemented into &lt;code&gt;R&lt;/code&gt;.
Running &lt;code&gt;pbinom(x = 54, size = 100, prob = .5, lower.tail = FALSE)&lt;/code&gt; (I will explain below) we can get R to calculate the proportion of samples that would result in &lt;em&gt;more than&lt;/em&gt; &lt;code&gt;x&lt;/code&gt; times heads in an experiment of &lt;code&gt;size&lt;/code&gt; 100 (i.e. tossing a coin 100 times).
Executing this in &lt;code&gt;R&lt;/code&gt; we get 0.18.&lt;/p&gt;
&lt;p&gt;In short, the &lt;code&gt;pbinom&lt;/code&gt; function gives us the answer to the question &#34;what is proportion of possible toss-sequences that results in more than &lt;code&gt;x = 54&lt;/code&gt;&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; heads if we toss a coin &lt;code&gt;size=100&lt;/code&gt; times if our hypothesis is that the coin is fair with a probability of heads of &lt;code&gt;prob = .5&lt;/code&gt;.
The &lt;code&gt;lower.tail = FALSE&lt;/code&gt; argument tells &lt;code&gt;R&lt;/code&gt; that we want the the probability of the upper part of the probability mass (i.e. the probability of getting 55 or &lt;em&gt;more&lt;/em&gt; heads rather than 54 or &lt;em&gt;less&lt;/em&gt;).
In other words, every 6th out of the &lt;span class=&#34;math inline&#34;&gt;\(2^{100}\)&lt;/span&gt; possible sequences has 55 or more heads.
Again, this is not really surprising and would probably not make us conclude that a coin is definitely unfair.&lt;/p&gt;
&lt;p&gt;If, for example, we think that a coin is unfair if the amount of heads (or more heads) has only a probability of .01 or 1 percent, what amount of heads would allow us to draw such a conclusion when tossing a coin 100 times?
To solve this, we can just use the &lt;code&gt;pbinom&lt;/code&gt; function for not only &lt;code&gt;x = 55&lt;/code&gt; heads but also 56 up to 100 heads, and see from which point onwards only 1 percent of all &lt;span class=&#34;math inline&#34;&gt;\(2^{100}\)&lt;/span&gt; sequences include so many heads.&lt;/p&gt;
&lt;p&gt;The code below does exactly this.
Conveniently the &lt;code&gt;pbinom&lt;/code&gt; function cannot only evaluate 1 value at the same time but we can just pass all values that we want to try as a sequence and it will give us the probability of &lt;code&gt;x&lt;/code&gt; heads for each of them so we can store them as a collection of values.
We can plot these values to see when we cross the 1 percent line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_heads &amp;lt;- 54:99  # all possible amount of heads that we want to try.

p_heads &amp;lt;- pbinom(q = n_heads, size = 100, prob = .5, lower.tail = F)  # get pbinom to show us the probability of so many heads for each of the values if a coin is fair

plot(n_heads, p_heads)
abline(h = .01)
abline(v = n_heads[p_heads &amp;lt; .01][1])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://julianquandt.com/post/2020-04-22-power-analysis-by-data-simulation-in-r-part-i_files/figure-html/caclulate_p1percent-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using the above we see that the at 62 tosses, i.e. the 9th element of the vector is the first probability that is smaller than .01, in this case 0.006.
In other words, getting 62 or more heads in 100 tosses would only happen extremely rarely, in 1 out of 62 cases.
Thus actually getting 62 heads is pretty surprising and if something like that happened we might conclude that a coin is unfair.&lt;/p&gt;
&lt;p&gt;However, instead of following the approach above, there is an easier way to get the number of heads that would surprise us by using the &lt;code&gt;qbinom&lt;/code&gt; function that gives us the quantile (i.e. number of heads or more heads) that would only happen with a certain probability: &lt;code&gt;qbinom(p = .01, size = 100, prob = .5, lower.tail = FALSE)&lt;/code&gt; = 62, unsurprisingly gives us the same result.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;back-to-power&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Back to Power&lt;/h1&gt;
&lt;p&gt;In this coin-toss example, we run a statistical test about the fairness of the coin.
But what is the &lt;strong&gt;power&lt;/strong&gt; of the test here (in case you forgot during my very long “brief” introduction, this post was about power)?&lt;/p&gt;
&lt;p&gt;Actually, we already used three concepts from Frequentist statistical testing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;null-hypothesis&lt;/strong&gt;: we assumed that the coin is fair&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;p-value&lt;/strong&gt; : the probability that we calculated above when checking how likely 62 or more heads are (we got 0.006).&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;alpha level&lt;/strong&gt; : (i.e. where people conventionally use .05 in psychological literature&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;) is the threshold that we picked for concluding it would surprise us enough to say that the coin must be unfair, i.e. the probability of .01. In other words, it is the chance that we conclude that a coin is unfair even if it is actually fair (the one percent of cases where 62 or more heads would happen even with a fair coin).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In a proper power analysis, all of these need to be specified in addition to an &lt;em&gt;alternative hypothesis&lt;/em&gt;. Let’s give it a try and specify them to run a power-analysis&lt;/p&gt;
&lt;div id=&#34;specifying-the-null-hypothesis.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Specifying the null-hypothesis.&lt;/h2&gt;
&lt;p&gt;Again, we will assume that a coin is fair and will produce heads with a probability of .50&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;specifying-the-alternative-hypothesis-aka-effect-size.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Specifying the Alternative Hypothesis aka Effect Size.&lt;/h2&gt;
&lt;p&gt;To conduct a proper power-analysis, it is important that we specify a concrete &lt;em&gt;alternative hypothesis&lt;/em&gt; (or effect size).
If we do know have a hypothesis about the effect size, we can &lt;em&gt;by definition&lt;/em&gt; not know the power to investigate this hypothesis.
Makes sense, right?&lt;/p&gt;
&lt;p&gt;This means we will try to think about an effect size that would be meaningful in this coin flip example.
For instance, if you use the coin to make an important decision, i.e. when the fairness of the coin is very important (see e.g. &lt;a id=&#34;Figure-3-Ref&#34; href=&#34;#Figure-3&#34;&gt;Figure 3&lt;/a&gt;)&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; you would probably want to be very strict about when a coin is unfair and would like to, for instance, already conclude that it is unfair at 55% percent heads, a deviation from a truly fair coin of 5%.&lt;/p&gt;
&lt;div class=&#34;notebox&#34;&gt;
&lt;p&gt;
Most software packages use effect-size estimates like Cohen’s d or f or other &lt;em&gt;standardized effect sizes&lt;/em&gt;.
We will have a look at how to do this with simulations briefly in the second part of the tutorial, but throughout this tutorial, we will mostly follow a different approach by trying to specify the expected effect size on the &lt;em&gt;raw&lt;/em&gt; scale.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;specifying-the-alpha-level&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Specifying the alpha-level&lt;/h2&gt;
&lt;p&gt;Furthermore, as it does not deem you a good idea to get into a fight with Harvey (&lt;a id=&#34;Figure-3-Ref&#34; href=&#34;#Figure-3&#34;&gt;Figure 3&lt;/a&gt;) by incorrectly accusing him of using an unfair coin, you want the chance of this happening (i.e. the alpha-level) to be very low at only 0.1 percent (1 in a 1000 cases)&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;specifying-the-desired-power&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Specifying the desired power&lt;/h2&gt;
&lt;p&gt;Moreover, you also want to be sure that you &lt;em&gt;would&lt;/em&gt; detect the unfairness of 55% if it is actually there.
Let’s say you only want to have a 10 percent chance of not detecting it if it was there.
100 minus this chance is the &lt;em&gt;power&lt;/em&gt; of our coin-toss study, i.e. 90 percent (or 1-.10 on the probability scale).&lt;/p&gt;
&lt;a id=&#34;Figure-3&#34;&gt;
&lt;figure&gt;
&lt;img src=&#34;https://media1.tenor.com/images/507d58a626fa15cb0b57e57f7a1cb873/tenor.gif?itemid=9698651&#34; width=&#34;500&#34;/&gt;
&lt;figcaption&gt;
&lt;em&gt;Figure 3. An illustrative example of when a coin-toss really matters.&lt;/em&gt;&lt;a href=&#34;#Figure-3-Ref&#34;&gt;↩︎&lt;/a&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To summarize, our test has the following properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;alpha-level = .001&lt;/li&gt;
&lt;li&gt;alternative hypothesis (aka effect size / fairness-criterion) = 55% heads&lt;/li&gt;
&lt;li&gt;power = .90&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our job now is to figure out at which number of tosses we can be 90% sure to detect the unfairness of 55% with only a 0.1% chance of getting into a fight with Harvey by wrongly accusing him of unfairness.&lt;/p&gt;
&lt;p&gt;To do this we can use the following r-code (I will explain below)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-first-power-calculation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The first Power Calculation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power_at_n &amp;lt;- c(0) # initialize vector that stores power for each number of tosses
n_heads &amp;lt;- c() # save &amp;quot;critical&amp;quot; number of heads for that toss-amount that would result 
n_toss &amp;lt;- 2 # initialize the toss-counter
while(power_at_n[n_toss-1] &amp;lt; .90){ # continue as long as power is not 90%
  n_heads[n_toss] &amp;lt;- qbinom(.001, n_toss, .5, lower.tail = F) # retrieve critical value
  power_at_n[n_toss] &amp;lt;- pbinom(n_heads[n_toss], n_toss, .55, lower.tail = F) # calculate power (1-beta) for each coin-toss
  n_toss &amp;lt;- n_toss+1 # increase toss-number 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above loop needs some explanation.
It increases the toss-amount &lt;code&gt;n_toss&lt;/code&gt; by 1 as long as it has not yet reached 90% power.
To do this, we again use the &lt;code&gt;qbinom&lt;/code&gt; function to find the number of heads (or more heads) that would only occur with a probability of .001.
In other words, only 0.1% of the possible coin toss sequences would result in that specific amount of heads when tossing the coin &lt;code&gt;n_toss&lt;/code&gt; times.
This is the same thing we did earlier with a fixed amount of 100 tosses.&lt;/p&gt;
&lt;p&gt;in the next line, we take this amount of heads that only occurs with a probabiltiy of .001 for the current toss amount &lt;code&gt;n_toss&lt;/code&gt; and use &lt;code&gt;pbinom&lt;/code&gt; to calculate the probability of getting &lt;em&gt;at least&lt;/em&gt; this many heads with our hypothesized &lt;em&gt;unfair&lt;/em&gt; coin that produces heads in 55% of the cases.
This means, we calculate the percentage of coin toss sequences that contain at least this many heads.
This probability is the &lt;strong&gt;power&lt;/strong&gt; of the test.
Why?
Because if 90% of the sequences contain 55% or more heads, if we take the coin and toss it &lt;code&gt;n_toss&lt;/code&gt; times,in 90% of the cases we will get one of those sequences that contain 55% or more heads.&lt;/p&gt;
&lt;p&gt;Lets have a look at two of the values from this calculation to make this more clear.
For instance, lets look at the values when the loop tried out 100 tosses:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;n_heads[100]&lt;/code&gt; = 65. This number is higher than the 62 heads we had above, as we are more strict now by specifying a stricter alpha level (.001 instead of .01).
Next, this 65 was passed on to the &lt;code&gt;pbinom&lt;/code&gt; function and we can look at the power, &lt;code&gt;power_at_n[100]&lt;/code&gt; = 0.02.
At this point we only have a power of .02 or 2%.
This means that tossing an unfair coin that would give 55% heads 100 times, only 2% of the sequences would contain 55 or more heads, therfore making it unlikely that we would detect the unfairness.
When we increase the number of tosses until the loop stops, we are at &lt;code&gt;n_toss-1&lt;/code&gt; = 1908 coin tosses.
At this number of tosses, the amount of heads that would make us conclude that a fair coin is unfair (biased with 55% heads) with only 0.1% chance of being wrong is &lt;code&gt;n_heads-[n_toss-1]&lt;/code&gt; = 1021.
Thus in this case, with 1908 tosses, if we get 1021 heads or more, we conclude that the coin is unfair.
What is the chance of getting at least that with our &lt;em&gt;unfair&lt;/em&gt; coin?
That’s what the &lt;code&gt;pbinom&lt;/code&gt; function in the loop above tells us and it is &lt;code&gt;power_at_n[n_toss-1]&lt;/code&gt; = 0.9, our specified 90%.
We can also plot the power for each number of tosses that we tried in the loop.
Figure 4 shows the increase in power with increasing sample-size.&lt;/p&gt;
&lt;a id=&#34;Figure-4&#34;&gt;
&lt;figure&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(1:(n_toss-1), power_at_n, xlab = &amp;quot;Number of coin-tosses&amp;quot;, ylab = &amp;quot;Power&amp;quot;, axes = FALSE)
abline(h = .90, col = &amp;quot;red&amp;quot;)
axis(side = 1, at = seq(0,(n_toss-1),by=100))
axis(side = 2, at = seq(0,1,by=0.1))&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://julianquandt.com/post/2020-04-22-power-analysis-by-data-simulation-in-r-part-i_files/figure-html/power-curve1-1.png&#34; width=&#34;672&#34; /&gt;
&lt;figcaption&gt;
&lt;em&gt;Figure 4. Change in power until we reach 90% indicated by the red horizontal line.&lt;/em&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thus, when tossing a coin 1908 times&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;, in 90% of the cases we will be able to tell it’s biased and can confidently confront Harvey like shown in Figure 5.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;https://i.redd.it/wyklemmf5zz11.png&#34; alt=&#34;confronting-harvey&#34; width=&#34;500&#34;/&gt;
&lt;figcaption&gt;
&lt;em&gt;Figure 5. Confronting Harvey and telling him what we think about his coin.&lt;/em&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;If you are not familiar with using these distribution functions (&lt;code&gt;qbinom&lt;/code&gt;, &lt;code&gt;pbinom&lt;/code&gt; etc.) in &lt;code&gt;R&lt;/code&gt;, this might have been a lot of new information but this is basically what we do in power analysis:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We specify a null-hypothesis, an alternative hypothesis, an alpha-level and a desired power,&lt;/li&gt;
&lt;li&gt;We try a small sample-size.&lt;/li&gt;
&lt;li&gt;We retrieve the critical value, (the number of heads that would be more surprising than what we specified as our alpha-level).&lt;/li&gt;
&lt;li&gt;We calculate the probability that the amount of heads that would make us reject the null-hypothesis would be observed with the unfair coin according to the alternative hypothesis.&lt;/li&gt;
&lt;li&gt;We stop as soon as this probability is equal to the desired power.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;finally-an-actual-power-simulation.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Finally, an actual power simulation.&lt;/h1&gt;
&lt;p&gt;So far, we have not done any simulation but have merely analytically derived the power by making use of the binomial probability mass function&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt;.
Thus, for easy toy-examples like this one we would not need to do a simulation.
However, as soon as we deal with real examples, it is much more difficult to make use of this approach and if we have several predictors in our model, or if we deal with mixed-effect models or hierarchical models (as we will do in part IV of this tutorial) the above method is not feasible anymore.&lt;/p&gt;
&lt;p&gt;What we can do however, for any model of any complexity and form, is to actually pretend we were repeatedly doing the experiment for each sample size and see how often we would be able to reject the null-hypothesis.
For instance we could toss a coin 20 times and test whether we would reject the null-hypothesis.
We could then repeat this process for 20 tosses very often, e.g. 1,000 times and see what the probability is that we would conclude that the null-hypothesis is false&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt;.
This is what we do in power-simulation.&lt;/p&gt;
&lt;p&gt;An obvious disadvantage is that instead of just calculating the power for each toss-amount (i.e. sample-size) only once, we need to try each toss-amount out 1,000 times.
Thus, simulation takes much longer than a regular power-calculation, especially with more complex models and high sample-sizes.
However, the advantage of the method is that we can just learn it once and adjust it for any situation that we will ever find ourselves in, not having to ever walk through tedious interfaces again, selecting arbitrary analyses and setting ever changing parameters to certain values.
Another advantage (that will be discussed in detail later) is that we do not need to specify a precise alternative hypothesis and test it for that single value, but that we can actually remain more vague about what our alternative hypothesis (i.e. the effect size that we expect) will be.
Oftentimes we do not know exactly what effect-size we can expect and we might like to tell the power-analysis about this uncertainty.&lt;/p&gt;
&lt;p&gt;At last, let us do a power-simulation for the above example.
Luckily we do not really have to toss a coin as &lt;code&gt;R&lt;/code&gt; can do that for us by using the &lt;code&gt;rbinom&lt;/code&gt; function, that will as often as we call it do a coin-tossing experiment for us with a specified sample-size.
Lets first see how the &lt;code&gt;rbinom&lt;/code&gt; function works if we would want to toss a coin 20 times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1) # make sure our simulation will give the same results if you try it
rbinom(n = 1, size = 20, prob = .50) # let r do 1 experiment with 20 coin tosses of a fair coin&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code, &lt;code&gt;R&lt;/code&gt; tossed a coin 20 times and it resulted in 9 heads.
We could repeat this experiment again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2) # make sure our simulation will give other results than before
rbinom(n = 1, size = 20, prob = .5) # run the experiment again&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, giving us 8 heads.
By increasing the first argument to the &lt;code&gt;rbinom&lt;/code&gt; function, we can tell &lt;code&gt;R&lt;/code&gt; to repeat this experiment more often.
Moreover, we can tell it to make use of an unfair coin directly, so we can directly put our alternative hypothesis in the simulation by changing the last number of the &lt;code&gt;rbinom&lt;/code&gt; function from .50 to .55, to do the same test as above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_heads &amp;lt;- rbinom(n = 1000, size = 20, prob = .55) # run 1,000 experiments, of 20 coin tosses each, at once
str(n_heads) # show structure of vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  int [1:1000] 12 12 11 8 13 8 7 10 10 14 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, &lt;code&gt;R&lt;/code&gt; repeated the 20 coin-toss experiment 1,000 times with an unfair coin of 55% chance of resulting in heads giving us 1,000 times the amount of heads that it got.
Let us again now test how big our power was in this case, again with an alpha-level of .001.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_heads &amp;lt;- pbinom(n_heads, 20, .50, lower.tail = F) # calculate the probability of observing this many heads if the coin would be fair (which it is not cause we simulated with 55% heads)

exp_power &amp;lt;- mean(p_heads &amp;lt; .001) # check where this chance drops below our alpha level&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first line in the above code does exactly what we did earlier, just with a little change.
We take the amounts of heads that we got from an unfair coin, and check in how many cases we would conclude - assuming that the coin would actually be fair, thus using .50 as the probability in &lt;code&gt;pbinom&lt;/code&gt; - that the observed amounts of heads is too unlikely for us to believe that the coin was fair.
We save these probabilities to a vector.
The second line calculates the observed power of our experiment.
To understand what it does, let us have a look at the vector p_heads it looks the following (here only the first 10 out of 1,000 values):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_heads[1:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.13158798 0.13158798 0.25172234 0.74827766 0.05765915 0.74827766
##  [7] 0.86841202 0.41190147 0.41190147 0.02069473&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As shown above, the &lt;code&gt;p_heads&lt;/code&gt; vector contains the probability of observing each amount of heads &lt;code&gt;n_heads&lt;/code&gt; from our 1,000 experiments assuming the experiment was done with a fair coin (which it was not).
Now we would like to check how many of these probabilities are at least as small as our alpha-level, i.e. surprising enough to conclude the coin was not fair.
We do this by checking for each value whether it was .001 or smaller (&lt;code&gt;p_heads &amp;lt; .001&lt;/code&gt;).
This will result in another vector of 0 when the condition is false and 1 when the condition is true&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt;.
Taking the mean of this vector will give us the probability of rejecting the null-hypothesis while we actually know it is incorrect (as we put the bias in the coin ourselves).
Again, this will give us the power.
In the present case with 20 coin-tosses the power is &lt;code&gt;exp_power&lt;/code&gt; = 0.004 or 0.4 percent.
This is obviously very low and not surprising given that we already know from the calculation above that we need a lot more coin-tosses than 20 to get the desired power of 90%.
To get to our desired power in this example, we need to change the code above so it will try different sample-sizes again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
exp_power_at_n &amp;lt;- c(0) # create a vector where we can store the power for each sample-size
n_toss_start &amp;lt;- 19 # start at 21 tosses
n_toss_loop &amp;lt;- 2 # additional number of tosses tried (above 20)
while(exp_power_at_n[n_toss_loop-1] &amp;lt; .90){ # continue increasing the sample-size until power = 90%
  n_toss &amp;lt;- n_toss_start+n_toss_loop # calculate the current number of tosses
  n_heads &amp;lt;- rbinom(1000, n_toss, .55) # run 1000 experiments for any given number of tosses and store number of heads
  p_heads &amp;lt;- pbinom(n_heads, n_toss, .50, lower.tail = F) # calculate the probability of getting at least that many heads if the coin would be fair 
  exp_power_at_n[n_toss_loop] &amp;lt;- mean(p_heads &amp;lt; .001) # calculate power by checking what proportion of the probabilities is smaller than or equal to our alpha-level
  n_toss_loop = n_toss_loop+1
}

exp_power_at_n &amp;lt;- exp_power_at_n[-1] # remove the first 0 that we used to populate the vector for the first iteration of the loop&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code is similar to what we have done earlier when we tried only 1 sample-size.
This time, we iterate over different sample-sizes in a loop and store the power for each in the vector &lt;code&gt;exp_power_at_n&lt;/code&gt;.
As in the earlier calculation, we can now use this to see how many tosses we would need by having a look at where the loop stopped, i.e. when it reached 90% power, which is at &lt;code&gt;n_toss-1&lt;/code&gt; = 1795 tosses at which the power was &lt;code&gt;exp_power_at_n[length(exp_power_at_n)]&lt;/code&gt; = 0.903.
We can also plot all these values again as done in Figure 6.&lt;/p&gt;
&lt;figure&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(21:n_toss, exp_power_at_n, xlab = &amp;quot;Number of coin-tosses&amp;quot;, ylab = &amp;quot;Power&amp;quot;, ylim = c(0,1), axes = FALSE)
abline(h = .90, col = &amp;quot;red&amp;quot;)
axis(side = 1, at = seq(0,(n_toss),by=100))
axis(side = 2, at = seq(0,1,by=0.1))&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://julianquandt.com/post/2020-04-22-power-analysis-by-data-simulation-in-r-part-i_files/figure-html/figure6-1.png&#34; width=&#34;672&#34; /&gt;
&lt;figcaption&gt;
&lt;em&gt;Figure 6. Observed power in the simulation.&lt;/em&gt;
&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;The shape of the line looks very similar to the earlier calculation, however the line appears to be thicker.
Moreover, it might be surprising that we did not get the same amount of tosses that we got from the calculation.
Actually they differ quite a lot (1908 vs. 1795).
This is due to the fact that even when running 1,000 experiments for each sample-size there is still imprecision in the simulation.
Each coin-flip is random and even if we repeat a experiment 1,000 times this randomness is still in there (think of how many possible sequences there would be!).
This randomness is why we cat a different number in the simulation and why the line in Figure 6 is thicker than in &lt;a href=&#34;#Figure-4&#34;&gt;Figure 4&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If we want to approach the results of the calculation more closely in our simulation (i.e. get a more precise power-estimate) we can increase the number of experiments that &lt;code&gt;R&lt;/code&gt; will run per sample-size (i.e. the number of simulations).
For example, we could repeat the simulation with 100,000 experiments per sample-size.
You have to be patient here, this already takes a few minutes maybe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
exp_power_at_n &amp;lt;- c(0) # create a vector where we can store the power for each sample-size
n_toss_start &amp;lt;- 19 # start at 21 tosses
n_toss_loop &amp;lt;- 2 # additional number of tosses tried (above 20)
while(exp_power_at_n[n_toss_loop-1] &amp;lt; .90){ # continue increasing the sample-size until power = 90%
  n_toss &amp;lt;- n_toss_start+n_toss_loop # calculate the current number of tosses
  n_heads &amp;lt;- rbinom(100000, n_toss, .55) # run 1000 experiments for any given number of tosses and store number of heads
  p_heads &amp;lt;- pbinom(n_heads, n_toss, .50, lower.tail = F) # calculate the probability of getting at least that many heads if the coin would be fair 
  exp_power_at_n[n_toss_loop] &amp;lt;- mean(p_heads &amp;lt; .001) # calculate power by checking what proportion of the probabilities is smaller than or equal to our alpha-level
  n_toss_loop = n_toss_loop+1
}

exp_power_at_n &amp;lt;- exp_power_at_n[-1] # remove the first 0 that we used to populate the vector for the first iteration of the loop&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you run this code, you will see that it took substantially longer than the previous simulation with only 1,000 repetitions.
If we now look at the outcome again we find that the number of tosses &lt;code&gt;n_toss-1&lt;/code&gt; = 1870 is already slightly closer to the calculated value but still not exactly the same.
In my personal opinion, this imprecision is both advantage of simulation as well as disadvantage.
It is a disadvantage in that it is less precise than the calculation above in theory.
However, it is an advantage as it adds some noise to the power-estimation process that is actually also present in real life.
This is, even if there is an effect in the population, each new sample will always be different which is also the case in the simulation.
If we look at Figure 7, we also see that now the power-increase follows a more straight line again that is very close to the one from the exact calculation and not as thick anymore as in Figure 6.&lt;/p&gt;
&lt;figure&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(21:n_toss, exp_power_at_n, xlab = &amp;quot;Number of coin-tosses&amp;quot;, ylab = &amp;quot;Power&amp;quot;, ylim = c(0,1), axes = FALSE)
abline(h = .90, col = &amp;quot;red&amp;quot;)
axis(side = 1, at = seq(0,(n_toss),by=100))
axis(side = 2, at = seq(0,1,by=0.1))&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://julianquandt.com/post/2020-04-22-power-analysis-by-data-simulation-in-r-part-i_files/figure-html/figure7-1.png&#34; width=&#34;672&#34; /&gt;
&lt;figcaption&gt;
&lt;em&gt;Figure 7. Power curve for a more precise simulation.&lt;/em&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this part of the tutorial I tried to bring us all on the same page about why we would do data or power simulation and how it is related to exact power calculation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In principle, in power calculation we count the amount of potential data sequences (e.g. possible sequences of HEADS-TAILS in coin-tosses) that could happen in an experiment of a specific size (e.g. amount of coin-tosses).&lt;/li&gt;
&lt;li&gt;Afterwards, we calculate the critical value (e.g. number of heads) that would make us reject the null-hypothesis (e.g. a coin is fair) as only a very small percentage of sequences (“small” is defined by the alpha-level here) would surpass the critical value.&lt;/li&gt;
&lt;li&gt;Finally, we calculate the percentage of sequences according to the alternative hypothesis (e.g. a coin is unfair with a bias of 55%) that would surpass the critical value.
This is the power of our test.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In power-simulation we do something very similar, but instead of calculating how many possible data sequences there are and how many of them result in a critical value, we just &lt;strong&gt;try&lt;/strong&gt; this out a lot of times until we get an approximation of how often we would observe data that are inconsistent with the null-hypothesis given our specified alpha-level.&lt;/p&gt;
&lt;p&gt;I hope you forgive my rather lengthy introduction on power-analysis before actually doing the simulation, as I thought it would be a good foundation before we move on to more complex and realistic situations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In part II of this tutorial, we will move on to these more realistic situations that we might actually be interested in psychology by looking at how we do simulations for t-tests.&lt;/li&gt;
&lt;li&gt;In part III we will continue with ANOVA and regression designs.&lt;/li&gt;
&lt;li&gt;In part IV we will look at mixed-effects/multilevel models.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;footnotes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Footnotes&lt;/h1&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The answer to the question - &lt;em&gt;Should we be surprised&lt;/em&gt; - is basically what we get when we do a statistical test and inspect the p-value.
Ok, this is not exactly true but p-values &lt;em&gt;can&lt;/em&gt; actually be expressed in terms of how surprised we should be about an observation, given a certain hypothesis (e.g. when we assume no difference between groups as the null-hypothesis).
To read an awesome explanation about this, look at &lt;a href=&#34;https://lesslikely.com/statistics/s-values/&#34;&gt;this&lt;/a&gt; really cool blog-post about &lt;em&gt;s-values&lt;/em&gt; by &lt;a href=&#34;https://twitter.com/dailyzad&#34;&gt;Zad Chow&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Why is the order important here?
Because orders do identify unique outcomes.
If we do not consider entire sequences, we might be inclined to think that there are only four possibilities: 3xHEADS, 2xHEADS, 1xHEADS and 0xHEADS, and that each of these events is equally likely.
However, how likely each of these events is depends on how many different sequences can produce it.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;I looked it up and apparently this number (a one followed by 30 zeros) is called a &lt;em&gt;Nonillion&lt;/em&gt;. It is so big, that apparently the number of bacterial cells on earth is estimated at &lt;a href=&#34;https://www.pnas.org/content/95/12/6578&#34;&gt;5 Nonillion&lt;/a&gt;.
So I do not recommend trying to write down all possibilities…&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;code&gt;R&lt;/code&gt; excludes the first number here, so that is why we start at 54 rather than 55&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Notice however, that this alpha-level of .05 implies that we are surprised enough if something only happens in 1 out of 20 cases.
There have, however, been repeated calls to &lt;a href=&#34;https://www.nature.com/articles/s41562%20017%200189%20z&#34;&gt;change the standard alpha-level&lt;/a&gt;, to &lt;a href=&#34;https://www.nature.com/articles/s41562-018-0311-x&#34;&gt;justify it based on the specific situation you are in&lt;/a&gt; or to &lt;a href=&#34;https://arxiv.org/abs/1709.07588&#34;&gt;abandon it all together&lt;/a&gt; alongside other ideas of using alternatives like the &lt;a href=&#34;https://link.springer.com/article/10.3758/s13423-011-0088-7&#34;&gt;Bayes-Factor&lt;/a&gt;.
For the most part, in this tutorial I will try to justify the alpha that I choose and to stay away from the “magical” .05 as I agree with the justification approach in that we should at least &lt;em&gt;try&lt;/em&gt; and think harder about the alpha-level and it’s meaning.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;It might be confusing that i say “specify” a null-hypothesis here.
Is the null-hypothesis not by definition that there is no difference, i.e. that the coin is fair at 50%.
Well, yes and no.
Of course assuming coin-fairness is the most logical thing to do here but in real life, no matter what research question you are investigating, you will close to &lt;em&gt;always&lt;/em&gt; find a difference between groups.
If you keep increasing the sample size, at some point the effect will always be significant, no matter how small the deviation is.
This is, we could even find the unfairness of the coin if it is only 51%.
But is this 51% really big enough to care about?
Maybe, maybe not, but we can define a &lt;em&gt;smallest effect size of interest&lt;/em&gt; and use &lt;em&gt;Equivalence testing&lt;/em&gt; in which the null-hypothesis is a certain range of small deviations from the actual point of no-difference in which we say that the effect is too small to care about.
Equivalence testing is not new but surprisingly unknown and/or uncommon in the psychological literature.
If you are interested in Equivalence Testing, you should check out the great &lt;a href=&#34;https://journals.sagepub.com/doi/10.1177/2515245918770963&#34;&gt;paper(s)&lt;/a&gt; and &lt;a href=&#34;http://daniellakens.blogspot.com/2018/08/equivalence-testing-and-second.html&#34;&gt;blog-post(s)&lt;/a&gt; about it by &lt;a href=&#34;http://daniellakens.blogspot.com/&#34;&gt;Daniel Lakens&lt;/a&gt; and colleagues.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;For those unfamiliar with this reference: Consider watching Dark Knight, it’s a great movie. In short, the displayed character, Harvey Dent, a former state lawyer, falls from grace and and loses his faith in the law system. He takes the law into his own hands and decides whether people will be sentenced (i.e. killed) by tossing a coin.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;Software like G*Power often uses different standardized effect sizes for different analyses.
However, in many cases standardized effect-sizes can be converted into each other.
If you ever need to do such a thing, &lt;a href=&#34;http://hauselin.com&#34;&gt;Hause Lin&lt;/a&gt; made a nice &lt;a href=&#34;http://escal.site/&#34;&gt;conversion app&lt;/a&gt;.&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;Thereby justifying our alpha-level in this toy example.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;This might be a good point to open G*Power (maybe for the last time ever), to see whether you get the same conclusions there (spoiler: you will, and if not, that only shows that software like this is not necessarily easier to use).&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;You can see that is it not a simulation as you will always get the &lt;em&gt;exact same&lt;/em&gt; result whenever you run the code above.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;In other words, instead of writing all possible coin-toss sequences down and counting how many of them would produce 55% heads with a certain amount of tosses, we could just run very many experiments in which we for example throw a coin 20 times.
If we repeat these 20 tosses 1,000 times, we get an approximation of how many heads the toss sequences produce on average.&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34;&gt;&lt;p&gt;Technically the vector contains FALSE instead of 0 and TRUE instead of 1. However, in R, and many other programming languages, the two are interchangeable, allowing us to calculate the mean in the same way.&lt;a href=&#34;#fnref13&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Confidence in Values and Value-based Choice @ ASPO 2019</title>
      <link>https://julianquandt.com/talk/confidence-in-vbd/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://julianquandt.com/talk/confidence-in-vbd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Extending the Statistical Toolbox @ BSI PhD-Day 2019</title>
      <link>https://julianquandt.com/talk/extending_stats_toolbox/</link>
      <pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://julianquandt.com/talk/extending_stats_toolbox/</guid>
      <description></description>
    </item>
    
    <item>
      <title>When Mere Action versus Inaction Leads to Robust Preference Change</title>
      <link>https://julianquandt.com/publication/chen-when-mere-action-2019/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://julianquandt.com/publication/chen-when-mere-action-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Updating the P-Curve Analysis of Carbine and Larson with Results from Preregistered Experiments</title>
      <link>https://julianquandt.com/publication/veling-updating-pcurve-analysis-2019/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://julianquandt.com/publication/veling-updating-pcurve-analysis-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Introduction to `brms` @ SIPS 2019</title>
      <link>https://julianquandt.com/talk/brms-at-sips/</link>
      <pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://julianquandt.com/talk/brms-at-sips/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Role of Attention in Explaining the No-Go Devaluation Effect: Effects on Appetitive Food Items</title>
      <link>https://julianquandt.com/publication/quandt-role-attention-explaining-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://julianquandt.com/publication/quandt-role-attention-explaining-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>https://julianquandt.com/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://julianquandt.com/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>https://julianquandt.com/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://julianquandt.com/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
