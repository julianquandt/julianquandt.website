[{"authors":["admin"],"categories":null,"content":"A PhD student at the Behavioural Science Institute\rat Radboud University\rin Nijmegen working with Harm Veling\r, Rob Holland\r, and Bernd Figner\r.\nIn my PhD project, I aim to increase our understanding confidence in value-based decision-making. The questions that keep me awake at night are:\n How do people retrieve value-information when they make a decision? How does this retrieval of information relate to people's confidence in their decisions? This is, what does high or low confidence in VBD reflect? Why are people confident about some choices while they are uncertain of others?  Aside from working on my dissertation, I also enjoy working on my skills in statistics, statistical modelling and programming (R and python). Moreover, I am an enthusiast of open science, open access, open source and open bars.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://julianquandt.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"A PhD student at the Behavioural Science Institute\rat Radboud University\rin Nijmegen working with Harm Veling\r, Rob Holland\r, and Bernd Figner\r.\nIn my PhD project, I aim to increase our understanding confidence in value-based decision-making. The questions that keep me awake at night are:\n How do people retrieve value-information when they make a decision? How does this retrieval of information relate to people's confidence in their decisions?","tags":null,"title":"Julian Quandt","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]]\rname = \u0026#34;Courses\u0026#34;\rurl = \u0026#34;courses/\u0026#34;\rweight = 50\rOr, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]]\rname = \u0026#34;Docs\u0026#34;\rurl = \u0026#34;docs/\u0026#34;\rweight = 50\rUpdate the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://julianquandt.com/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I'll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://julianquandt.com/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I'll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://julianquandt.com/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":["Power Analysis"],"content":"\r\r\rPower Analysis by Data Simulation in R - Part I: Theoretical introduction to simulation\rBack to Power\rFinally, an actual power simulation.\rSummary\rFootnotes\r\r\r\r\r\rClick below to download the .Rmd file\r\rDownload power-analysis-by-data-simulation-in-r-part-i.Rmd\rPower Analysis by Data Simulation in R - Part I: Theoretical introduction to simulation\rWhy this blog?\rIn recent years, power-analysis has become a standard tool in the behavioral sciences.\rWith an ongoing replication crisis, high-power research is the a key to improving replicability and to improving the reliability of findings.\rEspecially with preregistration slowly becoming standard practice in psychology, power-analysis, the practice of estimating the required sample-size a-priori, is a more important step than ever to plan research projects accordingly.\nNot so long ago, power-analysis was a rather complicated endeavor urging people to use algebraic solutions to calculate power of planned studies which can be demanding especially for non-trivial designs (i.e. basically anything that is not a correlation).\rLuckily, for many research designs power-analysis is nowadays readily available in software packages such as G*Power and even for relatively complex designs in specialized tools such as the great PANGEA tool for all kinds of generalized ANOVA designs and other tools by Jake Westfall.\nHowever, while these tools are really great they also have (in my personal opinion) two drawbacks.\rFirst, they urge the user to familiarize themselves with a new piece of software with new user interfaces, that are not always intuitive.\rSecond, and more importantly, these interfaces promote a statistical way of thinking that often leaves the user confused with what the ever-changing parameters (think of these \\(\\delta\\) , \\(d\\), \\(f\\), \\(f^2\\) and vague “group-size” and “measurement-point” fields) that need to be filled in mean.\rMoreover, these parameters differ for most designs and give an impression that power-analysis is complicated business better left to statisticians or that it might not be worth the effort.\n\r\rFigure 1. Me trying to figure out how to use a standard power-analysis software.\r\r\rHowever, this impression changed dramatically for me, once I changed my ways and started doing power analysis by simulation.\rMoreover, learning how to simulate data can of course not only be of use for power analysis but is a useful skill to in every research project.\rWhen we simulate data, we can see whether what we think about the data-generating process will actually result in the patterns that we would expect.\rIn other words, we can do theoretical experiments to see whether if everything goes as we would expect, we would also find the results that we would expect.\rEspecially when our analyses become more complex, all the different parts in our model tend to interact and it is easy to get lost.\rBy simulating data before conducting an experiment, we will be forced (and able) to specify more precisely how we think a theoretical model or prediction will be reflected in the data.\nThus, even though this tutorial will focus on simulating data for power analysis, you will learn a very useful skill on the side - simulating your own data and thereby conducting theoretical experiments before you even collect data.\rI know that there are already some excellent tutorials on data/power-simulation out there but they are often very brief and/or technical and assume a rather high level of prior knowledge about R and data-simulation in general.\rTherefore, I will spend some time on explaining theoretical concepts and slowly build up the simulation-code to hopefully enable the reader to understand the underlying principles and flexibly conduct power simulations themselves after reading this.\nThroughout the tutorial I assume readers are familiar with R and some of it’s base functionality.\rThis tutorial will consist of four different parts.\n\rIn the first part (the one you are reading) I will give a short overview of how power-analysis by simulation works on a conceptual level and why I prefer it to available power-analysis software, even though there are clear drawbacks that I will also briefly mention.\rMoreover, I will introduce the concept of power as I want to bring us all on the same page and want to give a (hopefully) intuitive example about what we actually do in power calculation and how it relates to the simulation techniques that we will use for the rest of the tutorial.\rIn the second part, I will discuss simulations for the simplest case of paired and two-sample t-tests.\rIn the third part, we will explore different ANOVA and regression designs.\rIn the fourth part we will move on to more complex mixed-effects and hierarchical models and even have a peak at Bayesian approaches to power analysis (well technically its not a power analysis but a true detection rate analysis).\rFor this fourth part, I assume readers will be familiar with how to fit mixed-effect models in lmer and/or brms.\r\rThis part of the tutorial is by far the most wordy and longest part.\rJust as a little motivator to keep in mind during this sometimes lengthy tutorial:\rAt the end of part IV of this tutorial, you will know how to do your own custom power analysis for mixed-effects models.\n\rPower Analysis by Simulation (a sustainable alternative)\r\r\rFigure 2. The proof that there are alternative ways to how we normally get our power.\r\r\r\rWhat is power again? A brief introduction\rAs my intention is to keep this post as short as possible (SPOILER: which definitely did not work), let’s directly dive into the topic by having a look at the definition of power:\nIf a certain effect of interest exists (e.g. a difference between two groups) power is the chance that we actually find the effect in a given study.\nTo provide some intuition about power, lets assume you toss a coin 10 times and you get 10 heads.\rShould you be surprised about this?\rIntuitively, it makes sense that we should be more surprised the more often we toss the coin and it keeps landing on head. 10 out of 10 heads would for example be less surprising than 10,000 out of 10,000, right?\rThis is exactly the question we want to answer when with power analysis1.\rHow often should we toss the coin until we are surprised enough to conclude that the coin is not fair.\nTo translate the above situation into a Frequentist null-hypothesis significance testing (NHST) scenario, we hypothesize that the coin is indeed fair (null-hypothesis) and see whether or not the observed number of heads (the observed data) fits with this hypothesis or not. If yes, we will retain the null-hypothesis that the coin is fair, if not, we will conclude that the data are very unlikely to result from tossing a fair coin.\nHow do we do this?\rWell, assume we observed 3 heads out of 3 total tosses.\rNow we can count all ways that this could have happened with a fair coin and compare it to all possible outcomes that our coin toss experiment might have produced.\rThese possibilities are (for a fair coin or any coin that cannot produce only heads or only tails):\n\r\r\rToss #1\rToss #2\rToss #3\rnumber of heads\r\r\r\rPossibility #1\rHEAD\rHEAD\rHEAD\r3\r\rPossibility #2\rHEAD\rHEAD\rTAIL\r2\r\rPossibility #3\rHEAD\rTAIL\rTAIL\r1\r\rPossibility #4\rTAIL\rTAIL\rTAIL\r0\r\rPossibility #5\rTAIL\rTAIL\rHEAD\r1\r\rPossibility #6\rTAIL\rHEAD\rHEAD\r2\r\rPossibility #7\rTAIL\rHEAD\rTAIL\r1\r\rPossibility #8\rHEAD\rTAIL\rHEAD\r2\r\r\r\rNotice that a possibility is not only defined by the number of heads and tails but also by the order in which they occur2. We can also calculate the number of possibilities as \\(2^x\\) where 2 means that we have 2 possible outcomes per toss and \\(x\\) is the number of tosses. In this case the number of possibilities is therefore \\(2^3 = 8\\).\rOnly 1 of these 8 events (Possibility 1) can produce 3 out of 3 heads.\rIf our coin is fair, each of these events should be equally likely and we can see that when flipping a fair coin 3 times, in only 1 out of 8 cases (12.5%) we will get 3 heads (Possibility 1).\nDo we find this surprising enough to conclude that the coin that we flipped is unfair?\rMaybe, maybe not.\rIf not, instead of flipping the coin three times, we could flip it 100 times.\rImagine we observed 55 heads in 100 flips.\rWe could now start writing down all possible outcomes, but that would take some time.\rThis time, there are not 8 but \\(2^{100}\\) = 1,267,651,000,000,000,000,000,000,000,000 possible sequences of heads and tails, and we would have to count the ones that produce at least 55 heads to see how often that would happen if our coin is fair3.\nHowever, luckily we are not the first who are interested in these kind of problems and we can make use of mathematical formulas that other people figured out for us.\nIn this case we need the binomial probability mass function.\rIn short, this function defines how often we can get each outcome, assuming a certain chance of getting heads or tails.\rIf you are interested in seeing this function and see how we can hand-code it in R, then click on the info-box below.\rIf you do not want to get too much into the technicalities here, you can also just read on.\n\n\r\r\rClick here to extend information about the binomial likelihood function\r\r\n\r\\[P(x)=\\frac{N!}{x!(N-x)!}\\pi^x(1-\\pi)^{N-x} \\]\nFor people who are not used to mathematical formulas, this might already look intimidating. However, all we need to know right now is that this formula gives us the probability of getting x heads \\(P{(x)}\\) (i.e. the number of ways we get x heads divided by N tosses):\rNote that \\(\\pi\\) in the above formula is not the one we might know from geometry but it is simply the Greek letter for p denoting a probability here.\rIn this case, it is the probability of either event, heads or tails, happening on each toss so it is 50% or 0.5.\rWe can fill this in for the example above:\n\\[P(55)=\\frac{100!}{55!(100-55)!}0.5^{55}(1-0.5)^{100-55}\\]\nTranslating this formula into R syntax we get the following:\nfactorial(100)/(factorial(55)*factorial(100-55))*0.5^55*(1-0.5)^(100-55) = 0.05\nHowever, before we said that we do not need the probability of exactly 55 heads but everything that is at least 55. In order to answer our question how often we get at least 55 heads, we could repeat the above calculation with all values from 55 up until 100 and add up the probabilities that we get.\nFor example we could do this with a for-loop:\n# first we write a function that calculates the probability for each number so we can call it in a loop\rpbinom2 \u0026lt;- function(N, x, p){\rfactorial(N)/(factorial(x)*factorial(N-x))*p^x*(1-p)^(N-x)\r}\rtosses_55to100 \u0026lt;- c(55:100) # we define the amount of heads that we want to check for (all bigger than or equal to 55)\rprobs \u0026lt;- c() # we will make an empty collection that we will add the results for each number of heads to\rfor(i in tosses_55to100){\rprobs \u0026lt;- append(probs, pbinom2(100,i,.5))\r}\rprint(probs[1:10])\r## [1] 0.048474297 0.038952560 0.030068643 0.022292270 0.015869073 0.010843867\r## [7] 0.007110732 0.004472880 0.002697928 0.001559739\rNow we got all the probabilities for each of the amounts of heads that we are interested in.\rBy summing them up we get what we need - the probability of getting at least 55 heads in 100 tosses, sum(probs) = 0.18 or 18 percent.\n\r\r\nAs mentioned, fortunately, the counting of possibilities is pre-implemented into R.\rRunning pbinom(x = 54, size = 100, prob = .5, lower.tail = FALSE) (I will explain below) we can get R to calculate the proportion of samples that would result in more than x times heads in an experiment of size 100 (i.e. tossing a coin 100 times).\rExecuting this in R we get 0.18.\nIn short, the pbinom function gives us the answer to the question \"what is proportion of possible toss-sequences that results in more than x = 544 heads if we toss a coin size=100 times if our hypothesis is that the coin is fair with a probability of heads of prob = .5.\rThe lower.tail = FALSE argument tells R that we want the the probability of the upper part of the probability mass (i.e. the probability of getting 55 or more heads rather than 54 or less).\rIn other words, every 6th out of the \\(2^{100}\\) possible sequences has 55 or more heads.\rAgain, this is not really surprising and would probably not make us conclude that a coin is definitely unfair.\nIf, for example, we think that a coin is unfair if the amount of heads (or more heads) has only a probability of .01 or 1 percent, what amount of heads would allow us to draw such a conclusion when tossing a coin 100 times?\rTo solve this, we can just use the pbinom function for not only x = 55 heads but also 56 up to 100 heads, and see from which point onwards only 1 percent of all \\(2^{100}\\) sequences include so many heads.\nThe code below does exactly this.\rConveniently the pbinom function cannot only evaluate 1 value at the same time but we can just pass all values that we want to try as a sequence and it will give us the probability of x heads for each of them so we can store them as a collection of values.\rWe can plot these values to see when we cross the 1 percent line.\nn_heads \u0026lt;- 54:99 # all possible amount of heads that we want to try.\rp_heads \u0026lt;- pbinom(q = n_heads, size = 100, prob = .5, lower.tail = F) # get pbinom to show us the probability of so many heads for each of the values if a coin is fair\rplot(n_heads, p_heads)\rabline(h = .01)\rabline(v = n_heads[p_heads \u0026lt; .01][1])\rUsing the above we see that the at 62 tosses, i.e. the 9th element of the vector is the first probability that is smaller than .01, in this case 0.006.\rIn other words, getting 62 or more heads in 100 tosses would only happen extremely rarely, in 1 out of 62 cases.\rThus actually getting 62 heads is pretty surprising and if something like that happened we might conclude that a coin is unfair.\nHowever, instead of following the approach above, there is an easier way to get the number of heads that would surprise us by using the qbinom function that gives us the quantile (i.e. number of heads or more heads) that would only happen with a certain probability: qbinom(p = .01, size = 100, prob = .5, lower.tail = FALSE) = 62, unsurprisingly gives us the same result.\n\r\rBack to Power\rIn this coin-toss example, we run a statistical test about the fairness of the coin.\rBut what is the power of the test here (in case you forgot during my very long “brief” introduction, this post was about power)?\nActually, we already used three concepts from Frequentist statistical testing:\n\rThe null-hypothesis: we assumed that the coin is fair\rThe p-value : the probability that we calculated above when checking how likely 62 or more heads are (we got 0.006).\rThe alpha level : (i.e. where people conventionally use .05 in psychological literature5) is the threshold that we picked for concluding it would surprise us enough to say that the coin must be unfair, i.e. the probability of .01. In other words, it is the chance that we conclude that a coin is unfair even if it is actually fair (the one percent of cases where 62 or more heads would happen even with a fair coin).\r\rIn a proper power analysis, all of these need to be specified in addition to an alternative hypothesis. Let’s give it a try and specify them to run a power-analysis\nSpecifying the null-hypothesis.\rAgain, we will assume that a coin is fair and will produce heads with a probability of .506.\n\rSpecifying the Alternative Hypothesis aka Effect Size.\rTo conduct a proper power-analysis, it is important that we specify a concrete alternative hypothesis (or effect size).\rIf we do know have a hypothesis about the effect size, we can by definition not know the power to investigate this hypothesis.\rMakes sense, right?\nThis means we will try to think about an effect size that would be meaningful in this coin flip example.\rFor instance, if you use the coin to make an important decision, i.e. when the fairness of the coin is very important (see e.g. Figure 3)7 you would probably want to be very strict about when a coin is unfair and would like to, for instance, already conclude that it is unfair at 55% percent heads, a deviation from a truly fair coin of 5%.\n\rMost software packages use effect-size estimates like Cohen’s d or f or other standardized effect sizes. We will have a look at how to do this with simulations briefly in the second part of the tutorial, but throughout this tutorial, we will mostly follow a different approach by trying to specify the expected effect size on the rawscale.\r\r\r8\n\rSpecifying the alpha-level\rFurthermore, as it does not deem you a good idea to get into a fight with Harvey (Figure 3) by incorrectly accusing him of using an unfair coin, you want the chance of this happening (i.e. the alpha-level) to be very low at only 0.1 percent (1 in a 1000 cases)9.\n\rSpecifying the desired power\rMoreover, you also want to be sure that you would detect the unfairness of 55% if it is actually there.\rLet’s say you only want to have a 10 percent chance of not detecting it if it was there.\r100 minus this chance is the power of our coin-toss study, i.e. 90 percent (or 1-.10 on the probability scale).\n\r\rFigure 3. An illustrative example of when a coin-toss really matters.↩︎\r\r\r\nTo summarize, our test has the following properties:\n\ralpha-level = .001\ralternative hypothesis (aka effect size / fairness-criterion) = 55% heads\rpower = .90\r\rOur job now is to figure out at which number of tosses we can be 90% sure to detect the unfairness of 55% with only a 0.1% chance of getting into a fight with Harvey by wrongly accusing him of unfairness.\nTo do this we can use the following r-code (I will explain below)\n\rThe first Power Calculation\rpower_at_n \u0026lt;- c(0) # initialize vector that stores power for each number of tosses\rn_heads \u0026lt;- c() # save \u0026quot;critical\u0026quot; number of heads for that toss-amount that would result n_toss \u0026lt;- 2 # initialize the toss-counter\rwhile(power_at_n[n_toss-1] \u0026lt; .90){ # continue as long as power is not 90%\rn_heads[n_toss] \u0026lt;- qbinom(.001, n_toss, .5, lower.tail = F) # retrieve critical value\rpower_at_n[n_toss] \u0026lt;- pbinom(n_heads[n_toss], n_toss, .55, lower.tail = F) # calculate power (1-beta) for each coin-toss\rn_toss \u0026lt;- n_toss+1 # increase toss-number }\rThe above loop needs some explanation.\rIt increases the toss-amount n_toss by 1 as long as it has not yet reached 90% power.\rTo do this, we again use the qbinom function to find the number of heads (or more heads) that would only occur with a probability of .001.\rIn other words, only 0.1% of the possible coin toss sequences would result in that specific amount of heads when tossing the coin n_toss times.\rThis is the same thing we did earlier with a fixed amount of 100 tosses.\nin the next line, we take this amount of heads that only occurs with a probabiltiy of .001 for the current toss amount n_toss and use pbinom to calculate the probability of getting at least this many heads with our hypothesized unfair coin that produces heads in 55% of the cases.\rThis means, we calculate the percentage of coin toss sequences that contain at least this many heads.\rThis probability is the power of the test.\rWhy?\rBecause if 90% of the sequences contain 55% or more heads, if we take the coin and toss it n_toss times,in 90% of the cases we will get one of those sequences that contain 55% or more heads.\nLets have a look at two of the values from this calculation to make this more clear.\rFor instance, lets look at the values when the loop tried out 100 tosses:\nn_heads[100] = 65. This number is higher than the 62 heads we had above, as we are more strict now by specifying a stricter alpha level (.001 instead of .01).\rNext, this 65 was passed on to the pbinom function and we can look at the power, power_at_n[100] = 0.02.\rAt this point we only have a power of .02 or 2%.\rThis means that tossing an unfair coin that would give 55% heads 100 times, only 2% of the sequences would contain 55 or more heads, therfore making it unlikely that we would detect the unfairness.\rWhen we increase the number of tosses until the loop stops, we are at n_toss-1 = 1908 coin tosses.\rAt this number of tosses, the amount of heads that would make us conclude that a fair coin is unfair (biased with 55% heads) with only 0.1% chance of being wrong is n_heads-[n_toss-1] = 1021.\rThus in this case, with 1908 tosses, if we get 1021 heads or more, we conclude that the coin is unfair.\rWhat is the chance of getting at least that with our unfair coin?\rThat’s what the pbinom function in the loop above tells us and it is power_at_n[n_toss-1] = 0.9, our specified 90%.\rWe can also plot the power for each number of tosses that we tried in the loop.\rFigure 4 shows the increase in power with increasing sample-size.\n\rplot(1:(n_toss-1), power_at_n, xlab = \u0026quot;Number of coin-tosses\u0026quot;, ylab = \u0026quot;Power\u0026quot;, axes = FALSE)\rabline(h = .90, col = \u0026quot;red\u0026quot;)\raxis(side = 1, at = seq(0,(n_toss-1),by=100))\raxis(side = 2, at = seq(0,1,by=0.1))\r\rFigure 4. Change in power until we reach 90% indicated by the red horizontal line.\r\r\r\nThus, when tossing a coin 1908 times10, in 90% of the cases we will be able to tell it’s biased and can confidently confront Harvey like shown in Figure 5.\n\r\rFigure 5. Confronting Harvey and telling him what we think about his coin.\r\r\rIf you are not familiar with using these distribution functions (qbinom, pbinom etc.) in R, this might have been a lot of new information but this is basically what we do in power analysis:\nWe specify a null-hypothesis, an alternative hypothesis, an alpha-level and a desired power,\rWe try a small sample-size.\rWe retrieve the critical value, (the number of heads that would be more surprising than what we specified as our alpha-level).\rWe calculate the probability that the amount of heads that would make us reject the null-hypothesis would be observed with the unfair coin according to the alternative hypothesis.\rWe stop as soon as this probability is equal to the desired power.\r\r\r\rFinally, an actual power simulation.\rSo far, we have not done any simulation but have merely analytically derived the power by making use of the binomial probability mass function11.\rThus, for easy toy-examples like this one we would not need to do a simulation.\rHowever, as soon as we deal with real examples, it is much more difficult to make use of this approach and if we have several predictors in our model, or if we deal with mixed-effect models or hierarchical models (as we will do in part IV of this tutorial) the above method is not feasible anymore.\nWhat we can do however, for any model of any complexity and form, is to actually pretend we were repeatedly doing the experiment for each sample size and see how often we would be able to reject the null-hypothesis.\rFor instance we could toss a coin 20 times and test whether we would reject the null-hypothesis.\rWe could then repeat this process for 20 tosses very often, e.g. 1,000 times and see what the probability is that we would conclude that the null-hypothesis is false12.\rThis is what we do in power-simulation.\nAn obvious disadvantage is that instead of just calculating the power for each toss-amount (i.e. sample-size) only once, we need to try each toss-amount out 1,000 times.\rThus, simulation takes much longer than a regular power-calculation, especially with more complex models and high sample-sizes.\rHowever, the advantage of the method is that we can just learn it once and adjust it for any situation that we will ever find ourselves in, not having to ever walk through tedious interfaces again, selecting arbitrary analyses and setting ever changing parameters to certain values.\rAnother advantage (that will be discussed in detail later) is that we do not need to specify a precise alternative hypothesis and test it for that single value, but that we can actually remain more vague about what our alternative hypothesis (i.e. the effect size that we expect) will be.\rOftentimes we do not know exactly what effect-size we can expect and we might like to tell the power-analysis about this uncertainty.\nAt last, let us do a power-simulation for the above example.\rLuckily we do not really have to toss a coin as R can do that for us by using the rbinom function, that will as often as we call it do a coin-tossing experiment for us with a specified sample-size.\rLets first see how the rbinom function works if we would want to toss a coin 20 times.\nset.seed(1) # make sure our simulation will give the same results if you try it\rrbinom(n = 1, size = 20, prob = .50) # let r do 1 experiment with 20 coin tosses of a fair coin\r## [1] 9\rIn the above code, R tossed a coin 20 times and it resulted in 9 heads.\rWe could repeat this experiment again:\nset.seed(2) # make sure our simulation will give other results than before\rrbinom(n = 1, size = 20, prob = .5) # run the experiment again\r## [1] 8\rIn this case, giving us 8 heads.\rBy increasing the first argument to the rbinom function, we can tell R to repeat this experiment more often.\rMoreover, we can tell it to make use of an unfair coin directly, so we can directly put our alternative hypothesis in the simulation by changing the last number of the rbinom function from .50 to .55, to do the same test as above.\nset.seed(1)\rn_heads \u0026lt;- rbinom(n = 1000, size = 20, prob = .55) # run 1,000 experiments, of 20 coin tosses each, at once\rstr(n_heads) # show structure of vector\r## int [1:1000] 12 12 11 8 13 8 7 10 10 14 ...\rNow, R repeated the 20 coin-toss experiment 1,000 times with an unfair coin of 55% chance of resulting in heads giving us 1,000 times the amount of heads that it got.\rLet us again now test how big our power was in this case, again with an alpha-level of .001.\np_heads \u0026lt;- pbinom(n_heads, 20, .50, lower.tail = F) # calculate the probability of observing this many heads if the coin would be fair (which it is not cause we simulated with 55% heads)\rexp_power \u0026lt;- mean(p_heads \u0026lt; .001) # check where this chance drops below our alpha level\rThe first line in the above code does exactly what we did earlier, just with a little change.\rWe take the amounts of heads that we got from an unfair coin, and check in how many cases we would conclude - assuming that the coin would actually be fair, thus using .50 as the probability in pbinom - that the observed amounts of heads is too unlikely for us to believe that the coin was fair.\rWe save these probabilities to a vector.\rThe second line calculates the observed power of our experiment.\rTo understand what it does, let us have a look at the vector p_heads it looks the following (here only the first 10 out of 1,000 values):\np_heads[1:10]\r## [1] 0.13158798 0.13158798 0.25172234 0.74827766 0.05765915 0.74827766\r## [7] 0.86841202 0.41190147 0.41190147 0.02069473\rAs shown above, the p_heads vector contains the probability of observing each amount of heads n_heads from our 1,000 experiments assuming the experiment was done with a fair coin (which it was not).\rNow we would like to check how many of these probabilities are at least as small as our alpha-level, i.e. surprising enough to conclude the coin was not fair.\rWe do this by checking for each value whether it was .001 or smaller (p_heads \u0026lt; .001).\rThis will result in another vector of 0 when the condition is false and 1 when the condition is true13.\rTaking the mean of this vector will give us the probability of rejecting the null-hypothesis while we actually know it is incorrect (as we put the bias in the coin ourselves).\rAgain, this will give us the power.\rIn the present case with 20 coin-tosses the power is exp_power = 0.004 or 0.4 percent.\rThis is obviously very low and not surprising given that we already know from the calculation above that we need a lot more coin-tosses than 20 to get the desired power of 90%.\rTo get to our desired power in this example, we need to change the code above so it will try different sample-sizes again.\nset.seed(1)\rexp_power_at_n \u0026lt;- c(0) # create a vector where we can store the power for each sample-size\rn_toss_start \u0026lt;- 19 # start at 21 tosses\rn_toss_loop \u0026lt;- 2 # additional number of tosses tried (above 20)\rwhile(exp_power_at_n[n_toss_loop-1] \u0026lt; .90){ # continue increasing the sample-size until power = 90%\rn_toss \u0026lt;- n_toss_start+n_toss_loop # calculate the current number of tosses\rn_heads \u0026lt;- rbinom(1000, n_toss, .55) # run 1000 experiments for any given number of tosses and store number of heads\rp_heads \u0026lt;- pbinom(n_heads, n_toss, .50, lower.tail = F) # calculate the probability of getting at least that many heads if the coin would be fair exp_power_at_n[n_toss_loop] \u0026lt;- mean(p_heads \u0026lt; .001) # calculate power by checking what proportion of the probabilities is smaller than or equal to our alpha-level\rn_toss_loop = n_toss_loop+1\r}\rexp_power_at_n \u0026lt;- exp_power_at_n[-1] # remove the first 0 that we used to populate the vector for the first iteration of the loop\rThe above code is similar to what we have done earlier when we tried only 1 sample-size.\rThis time, we iterate over different sample-sizes in a loop and store the power for each in the vector exp_power_at_n.\rAs in the earlier calculation, we can now use this to see how many tosses we would need by having a look at where the loop stopped, i.e. when it reached 90% power, which is at n_toss-1 = 1795 tosses at which the power was exp_power_at_n[length(exp_power_at_n)] = 0.903.\rWe can also plot all these values again as done in Figure 6.\n\rplot(21:n_toss, exp_power_at_n, xlab = \u0026quot;Number of coin-tosses\u0026quot;, ylab = \u0026quot;Power\u0026quot;, ylim = c(0,1), axes = FALSE)\rabline(h = .90, col = \u0026quot;red\u0026quot;)\raxis(side = 1, at = seq(0,(n_toss),by=100))\raxis(side = 2, at = seq(0,1,by=0.1))\r\rFigure 6. Observed power in the simulation.\r\r\rThe shape of the line looks very similar to the earlier calculation, however the line appears to be thicker.\rMoreover, it might be surprising that we did not get the same amount of tosses that we got from the calculation.\rActually they differ quite a lot (1908 vs. 1795).\rThis is due to the fact that even when running 1,000 experiments for each sample-size there is still imprecision in the simulation.\rEach coin-flip is random and even if we repeat a experiment 1,000 times this randomness is still in there (think of how many possible sequences there would be!).\rThis randomness is why we cat a different number in the simulation and why the line in Figure 6 is thicker than in Figure 4.\nIf we want to approach the results of the calculation more closely in our simulation (i.e. get a more precise power-estimate) we can increase the number of experiments that R will run per sample-size (i.e. the number of simulations).\rFor example, we could repeat the simulation with 100,000 experiments per sample-size.\rYou have to be patient here, this already takes a few minutes maybe.\nset.seed(1)\rexp_power_at_n \u0026lt;- c(0) # create a vector where we can store the power for each sample-size\rn_toss_start \u0026lt;- 19 # start at 21 tosses\rn_toss_loop \u0026lt;- 2 # additional number of tosses tried (above 20)\rwhile(exp_power_at_n[n_toss_loop-1] \u0026lt; .90){ # continue increasing the sample-size until power = 90%\rn_toss \u0026lt;- n_toss_start+n_toss_loop # calculate the current number of tosses\rn_heads \u0026lt;- rbinom(100000, n_toss, .55) # run 1000 experiments for any given number of tosses and store number of heads\rp_heads \u0026lt;- pbinom(n_heads, n_toss, .50, lower.tail = F) # calculate the probability of getting at least that many heads if the coin would be fair exp_power_at_n[n_toss_loop] \u0026lt;- mean(p_heads \u0026lt; .001) # calculate power by checking what proportion of the probabilities is smaller than or equal to our alpha-level\rn_toss_loop = n_toss_loop+1\r}\rexp_power_at_n \u0026lt;- exp_power_at_n[-1] # remove the first 0 that we used to populate the vector for the first iteration of the loop\rIf you run this code, you will see that it took substantially longer than the previous simulation with only 1,000 repetitions.\rIf we now look at the outcome again we find that the number of tosses n_toss-1 = 1870 is already slightly closer to the calculated value but still not exactly the same.\rIn my personal opinion, this imprecision is both advantage of simulation as well as disadvantage.\rIt is a disadvantage in that it is less precise than the calculation above in theory.\rHowever, it is an advantage as it adds some noise to the power-estimation process that is actually also present in real life.\rThis is, even if there is an effect in the population, each new sample will always be different which is also the case in the simulation.\rIf we look at Figure 7, we also see that now the power-increase follows a more straight line again that is very close to the one from the exact calculation and not as thick anymore as in Figure 6.\n\rplot(21:n_toss, exp_power_at_n, xlab = \u0026quot;Number of coin-tosses\u0026quot;, ylab = \u0026quot;Power\u0026quot;, ylim = c(0,1), axes = FALSE)\rabline(h = .90, col = \u0026quot;red\u0026quot;)\raxis(side = 1, at = seq(0,(n_toss),by=100))\raxis(side = 2, at = seq(0,1,by=0.1))\r\rFigure 7. Power curve for a more precise simulation.\r\r\r\rSummary\rIn this part of the tutorial I tried to bring us all on the same page about why we would do data or power simulation and how it is related to exact power calculation.\n\rIn principle, in power calculation we count the amount of potential data sequences (e.g. possible sequences of HEADS-TAILS in coin-tosses) that could happen in an experiment of a specific size (e.g. amount of coin-tosses).\rAfterwards, we calculate the critical value (e.g. number of heads) that would make us reject the null-hypothesis (e.g. a coin is fair) as only a very small percentage of sequences (“small” is defined by the alpha-level here) would surpass the critical value.\rFinally, we calculate the percentage of sequences according to the alternative hypothesis (e.g. a coin is unfair with a bias of 55%) that would surpass the critical value.\rThis is the power of our test.\r\rIn power-simulation we do something very similar, but instead of calculating how many possible data sequences there are and how many of them result in a critical value, we just try this out a lot of times until we get an approximation of how often we would observe data that are inconsistent with the null-hypothesis given our specified alpha-level.\nI hope you forgive my rather lengthy introduction on power-analysis before actually doing the simulation, as I thought it would be a good foundation before we move on to more complex and realistic situations.\n\rIn part II of this tutorial, we will move on to these more realistic situations that we might actually be interested in psychology by looking at how we do simulations for t-tests.\rIn part III we will continue with ANOVA and regression designs.\rIn part IV we will look at mixed-effects/multilevel models.\r\r\rFootnotes\r\r\rThe answer to the question - Should we be surprised - is basically what we get when we do a statistical test and inspect the p-value.\rOk, this is not exactly true but p-values can actually be expressed in terms of how surprised we should be about an observation, given a certain hypothesis (e.g. when we assume no difference between groups as the null-hypothesis).\rTo read an awesome explanation about this, look at this really cool blog-post about s-values by Zad Chow.↩︎\n\rWhy is the order important here?\rBecause orders do identify unique outcomes.\rIf we do not consider entire sequences, we might be inclined to think that there are only four possibilities: 3xHEADS, 2xHEADS, 1xHEADS and 0xHEADS, and that each of these events is equally likely.\rHowever, how likely each of these events is depends on how many different sequences can produce it.↩︎\n\rI looked it up and apparently this number (a one followed by 30 zeros) is called a Nonillion. It is so big, that apparently the number of bacterial cells on earth is estimated at 5 Nonillion.\rSo I do not recommend trying to write down all possibilities…↩︎\n\rR excludes the first number here, so that is why we start at 54 rather than 55↩︎\n\rNotice however, that this alpha-level of .05 implies that we are surprised enough if something only happens in 1 out of 20 cases.\rThere have, however, been repeated calls to change the standard alpha-level, to justify it based on the specific situation you are in or to abandon it all together alongside other ideas of using alternatives like the Bayes-Factor.\rFor the most part, in this tutorial I will try to justify the alpha that I choose and to stay away from the “magical” .05 as I agree with the justification approach in that we should at least try and think harder about the alpha-level and it’s meaning.↩︎\n\rIt might be confusing that i say “specify” a null-hypothesis here.\rIs the null-hypothesis not by definition that there is no difference, i.e. that the coin is fair at 50%.\rWell, yes and no.\rOf course assuming coin-fairness is the most logical thing to do here but in real life, no matter what research question you are investigating, you will close to always find a difference between groups.\rIf you keep increasing the sample size, at some point the effect will always be significant, no matter how small the deviation is.\rThis is, we could even find the unfairness of the coin if it is only 51%.\rBut is this 51% really big enough to care about?\rMaybe, maybe not, but we can define a smallest effect size of interest and use Equivalence testing in which the null-hypothesis is a certain range of small deviations from the actual point of no-difference in which we say that the effect is too small to care about.\rEquivalence testing is not new but surprisingly unknown and/or uncommon in the psychological literature.\rIf you are interested in Equivalence Testing, you should check out the great paper(s) and blog-post(s) about it by Daniel Lakens and colleagues.↩︎\n\rFor those unfamiliar with this reference: Consider watching Dark Knight, it’s a great movie. In short, the displayed character, Harvey Dent, a former state lawyer, falls from grace and and loses his faith in the law system. He takes the law into his own hands and decides whether people will be sentenced (i.e. killed) by tossing a coin.↩︎\n\rSoftware like G*Power often uses different standardized effect sizes for different analyses.\rHowever, in many cases standardized effect-sizes can be converted into each other.\rIf you ever need to do such a thing, Hause Lin made a nice conversion app.↩︎\n\rThereby justifying our alpha-level in this toy example.↩︎\n\rThis might be a good point to open G*Power (maybe for the last time ever), to see whether you get the same conclusions there (spoiler: you will, and if not, that only shows that software like this is not necessarily easier to use).↩︎\n\rYou can see that is it not a simulation as you will always get the exact same result whenever you run the code above.↩︎\n\rIn other words, instead of writing all possible coin-toss sequences down and counting how many of them would produce 55% heads with a certain amount of tosses, we could just run very many experiments in which we for example throw a coin 20 times.\rIf we repeat these 20 tosses 1,000 times, we get an approximation of how many heads the toss sequences produce on average.↩︎\n\rTechnically the vector contains FALSE instead of 0 and TRUE instead of 1. However, in R, and many other programming languages, the two are interchangeable, allowing us to calculate the mean in the same way.↩︎\n\r\r\r","date":1587513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587729644,"objectID":"aa86b66bc5d65f818c902d58df6501e8","permalink":"https://julianquandt.com/post/power-analysis-by-data-simulation-in-r-part-i/","publishdate":"2020-04-22T00:00:00Z","relpermalink":"/post/power-analysis-by-data-simulation-in-r-part-i/","section":"post","summary":"This part provides an introduction, some background on power-calculation and data-simulation.","tags":["power","data-simulation"],"title":"Power Analysis by Data Simulation in R - Part I","type":"post"},{"authors":["Julian Quandt"],"categories":["statistics"],"content":"","date":1573171200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579194048,"objectID":"aa779006cb0736ec33787a75d1417ba2","permalink":"https://julianquandt.com/talk/extending_stats_toolbox/","publishdate":"2019-11-08T00:00:00Z","relpermalink":"/talk/extending_stats_toolbox/","section":"talk","summary":"What we often forget when making use of statistics in our daily research practice is that statistics itself is a rapidly developing research field that is rapidly progressing. For instance, in recent yearsBayesFactors have been proposed asan alternative to p-values and their adaptation has quickly increased in psychological papers in recent years with many scholars claiming that they should become the standard way of statistical inference. However, while people start to adopt Bayes Factors quickly, there are problems and challenges that they might often go unnoticed. Furthermore, many Bayesian statisticians themselves say that we should not use p-values and BayesFactorsand suggest yet other alternatives.","tags":["statistics","statistical testing","p-value","bayes factor"],"title":"Extending the Statistical Toolbox @ BSI PhD-Day 2019","type":"talk"},{"authors":["Zhang Chen","Rob W. Holland","Julian Quandt","Ap Dijksterhuis","Harm Veling"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"7cfbd38a9c552d6c4dfc1cdf7de74141","permalink":"https://julianquandt.com/publication/chen-when-mere-action-2019/","publishdate":"2020-01-16T16:26:19.362758Z","relpermalink":"/publication/chen-when-mere-action-2019/","section":"publication","summary":"","tags":null,"title":"When Mere Action versus Inaction Leads to Robust Preference Change","type":"publication"},{"authors":["Harm Veling","Zhang Chen","Huaiyu Liu","Julian Quandt","Rob W. Holland"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"1b2553401cf029448bba13b9e939c909","permalink":"https://julianquandt.com/publication/veling-updating-pcurve-analysis-2019/","publishdate":"2020-01-16T16:31:32.225589Z","relpermalink":"/publication/veling-updating-pcurve-analysis-2019/","section":"publication","summary":"","tags":null,"title":"Updating the P-Curve Analysis of Carbine and Larson with Results from Preregistered Experiments","type":"publication"},{"authors":["Johannes Algermissen","Julian Quandt"],"categories":["statistics"],"content":"","date":1562630400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579193578,"objectID":"8ef08ea32bf0460a19786a9bf3634789","permalink":"https://julianquandt.com/talk/brms-at-sips/","publishdate":"2019-07-09T00:00:00Z","relpermalink":"/talk/brms-at-sips/","section":"talk","summary":"In statistics classes, students often learn a zoo of different models, without grasping how those all link together. In consequence, they often fit several models, one for each research question. Hierarchical/ multi-level/ mixed-effects models constitute a unifying framework that allows to address many questions in a single model, including questions that are not easily answered with standard ANOVAs (e.g. trial-by-trial effects). However, this increased flexibility comes at the costs of more complex computation. We will discuss the logic and benefits/ pitfalls of using mixed-effects models, and then introduce Markov chain Monte Carlo (MCMC) as a particularly suited fitting algorithm. We will introduce the R package brms, an easy-to-handle wrapper for fitting Bayesian mixed-effects models using MCMCs in the language Stan.","tags":["brms","bayesian statistics","mixed-effect models"],"title":"Introduction to `brms` @ SIPS 2019","type":"talk"},{"authors":["Julian Quandt","Rob W. Holland","Zhang Chen","Harm Veling"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3651ca96f085e6b7d89bca96b635b1b5","permalink":"https://julianquandt.com/publication/quandt-role-attention-explaining-2019/","publishdate":"2020-01-16T16:26:19.364752Z","relpermalink":"/publication/quandt-role-attention-explaining-2019/","section":"publication","summary":"","tags":["Appetite","Attention","Food","Motor Processes","Perceptual Motor Learning","Responses","Training"],"title":"The Role of Attention in Explaining the No-Go Devaluation Effect: Effects on Appetitive Food Items","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://julianquandt.com/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://julianquandt.com/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]